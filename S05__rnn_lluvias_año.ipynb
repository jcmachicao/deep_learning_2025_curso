{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPRcXoReGa7GAy7xWJb4Lv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcmachicao/deep_learning_2025_curso/blob/main/S05__rnn_lluvias_a%C3%B1o.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Seq2Seq: Predecir 30 días de precipitación (PyTorch)\n",
        "### Notebook preparado para Google Colab / ejecución local."
      ],
      "metadata": {
        "id": "qDNx43rmz2O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zebSmlCYz0wm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQeKC-5nzxCo"
      },
      "outputs": [],
      "source": [
        "# ------------------------------\n",
        "# 0. Config\n",
        "# ------------------------------\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ------------------------------\n",
        "# 1. Dataset sintético (estacional + ruido)\n",
        "# ------------------------------\n",
        "# Queremos generar muchas series anuales (365 días) y predecir los próximos 30 días.\n",
        "INPUT_LEN = 365\n",
        "TARGET_LEN = 30\n",
        "SAMPLES = 1200  # cantidad de ejemplos sintéticos\n",
        "\n",
        "# Generador: temporada anual + pequeñas variaciones por \"año\"\n",
        "def generate_yearly_precipitation(seed_offset=0):\n",
        "    # Base estacional: mezcla de senos para simular estaciones y eventos\n",
        "    days = np.arange(INPUT_LEN + TARGET_LEN)\n",
        "    # componente anual (estacionalidad fuerte)\n",
        "    annual = 10.0 * (np.sin(2 * np.pi * days / 365 - 0.2) + 1.0)\n",
        "    # componente subanual (meses húmedos intermitentes)\n",
        "    monthly = 3.0 * np.sin(2 * np.pi * days / 30 + 0.5)\n",
        "    # tendencia aleatoria por \"año\"\n",
        "    rnd = 2.0 * np.random.randn(INPUT_LEN + TARGET_LEN) * 0.5\n",
        "    # pequeños picos de lluvia aleatorios\n",
        "    spikes = np.where(np.random.rand(INPUT_LEN + TARGET_LEN) > 0.98, np.random.rand(INPUT_LEN + TARGET_LEN) * 20, 0.0)\n",
        "\n",
        "    series = np.maximum(0.0, annual + monthly + rnd + spikes)\n",
        "    # añadir pequeña variación dependiente del offset para crear diversidad entre series\n",
        "    series *= (1.0 + 0.05 * np.sin(seed_offset * 0.37))\n",
        "    return series.astype(np.float32)\n",
        "\n",
        "# Construir dataset con ventana (cada ejemplo: 365 in -> 30 out)\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(SAMPLES):\n",
        "    series = generate_yearly_precipitation(i)\n",
        "    x = series[:INPUT_LEN]\n",
        "    y = series[INPUT_LEN:INPUT_LEN+TARGET_LEN]\n",
        "    # normalizar por serie (opcional): media/std\n",
        "    mu = x.mean()\n",
        "    sigma = x.std() + 1e-6\n",
        "    x_norm = (x - mu) / sigma\n",
        "    y_norm = (y - mu) / sigma\n",
        "    X.append(x_norm[:, None])  # shape (365, 1)\n",
        "    Y.append(y_norm[:, None])  # shape (30, 1)\n",
        "\n",
        "X = np.stack(X)  # (SAMPLES, 365, 1)\n",
        "Y = np.stack(Y)  # (SAMPLES, 30, 1)\n",
        "\n",
        "# dividir train/val/test\n",
        "n_train = int(0.8 * SAMPLES)\n",
        "X_train, Y_train = X[:n_train], Y[:n_train]\n",
        "X_val, Y_val = X[n_train:int(0.9*SAMPLES)], Y[n_train:int(0.9*SAMPLES)]\n",
        "X_test, Y_test = X[int(0.9*SAMPLES):], Y[int(0.9*SAMPLES):]\n",
        "\n",
        "print('Shapes ->', X_train.shape, Y_train.shape, X_val.shape, X_test.shape)\n",
        "\n",
        "# ------------------------------\n",
        "# 2. Dataset y DataLoader\n",
        "# ------------------------------\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.Y = torch.from_numpy(Y).float()\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "BATCH = 32\n",
        "train_loader = DataLoader(TimeSeriesDataset(X_train, Y_train), batch_size=BATCH, shuffle=True)\n",
        "val_loader = DataLoader(TimeSeriesDataset(X_val, Y_val), batch_size=BATCH)\n",
        "\n",
        "# ------------------------------\n",
        "# 3. Model: Encoder-Decoder RNN simple\n",
        "# ------------------------------\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, input_size)\n",
        "        out, h_n = self.rnn(x)\n",
        "        return h_n  # (num_layers, batch, hidden_size)\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size=1, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, decoder_inputs, hidden):\n",
        "        # decoder_inputs: (batch, tgt_len, input_size)\n",
        "        out, h_n = self.rnn(decoder_inputs, hidden)\n",
        "        # proyectar cada paso\n",
        "        out2 = self.fc(out)  # (batch, tgt_len, output_size)\n",
        "        return out2, h_n\n",
        "\n",
        "# Modelo completo wrapper\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, target_len, device):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderRNN(input_size, hidden_size)\n",
        "        # para decoder usaremos el mismo input_size (1) ya que alimentamos el valor anterior\n",
        "        self.decoder = DecoderRNN(input_size=1, hidden_size=hidden_size, output_size=1)\n",
        "        self.target_len = target_len\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, tgt=None, teacher_forcing_ratio=0.5):\n",
        "        # src: (batch, INPUT_LEN, 1)\n",
        "        batch_size = src.size(0)\n",
        "        hidden = self.encoder(src)  # (num_layers, batch, hidden)\n",
        "\n",
        "        # Inicializamos el input del decoder con el último valor del src\n",
        "        # alternativa: usar un token <sos>. Aquí usamos el último día observado.\n",
        "        last_day = src[:, -1:, :]  # (batch, 1, 1)\n",
        "        decoder_input = last_day\n",
        "\n",
        "        outputs = torch.zeros(batch_size, self.target_len, 1, device=self.device)\n",
        "\n",
        "        for t in range(self.target_len):\n",
        "            out, hidden = self.decoder(decoder_input, hidden)\n",
        "            # out: (batch, 1, 1)\n",
        "            outputs[:, t:t+1, :] = out\n",
        "\n",
        "            # determinar el siguiente input: teacher forcing o predicción\n",
        "            if tgt is not None and random.random() < teacher_forcing_ratio:\n",
        "                # usar el valor verdadero (tgt) durante entrenamiento\n",
        "                decoder_input = tgt[:, t:t+1, :]\n",
        "            else:\n",
        "                # usar la propia predicción\n",
        "                decoder_input = out.detach()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Instanciar\n",
        "INPUT_SIZE = 1\n",
        "HIDDEN_SIZE = 64\n",
        "model = Seq2Seq(INPUT_SIZE, HIDDEN_SIZE, TARGET_LEN, DEVICE).to(DEVICE)\n",
        "\n",
        "# ------------------------------\n",
        "# 4. Entrenamiento\n",
        "# ------------------------------\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        yb = yb.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(xb, tgt=yb, teacher_forcing_ratio=0.5)\n",
        "        loss = criterion(out, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "    epoch_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # val\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            yb = yb.to(DEVICE)\n",
        "            out = model(xb, tgt=None, teacher_forcing_ratio=0.0)\n",
        "            val_loss += criterion(out, yb).item() * xb.size(0)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d}  Train Loss: {epoch_loss:.6f}  Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "# ------------------------------\n",
        "# 5. Guardar y cargar pesos\n",
        "# ------------------------------\n",
        "torch.save(model.state_dict(), 'seq2seq_rnn.pth')\n",
        "\n",
        "model2 = Seq2Seq(INPUT_SIZE, HIDDEN_SIZE, TARGET_LEN, DEVICE).to(DEVICE)\n",
        "model2.load_state_dict(torch.load('seq2seq_rnn.pth', map_location=DEVICE))\n",
        "model2.eval()\n",
        "\n",
        "# ------------------------------\n",
        "# 6. Inferencia y ejemplo gráfico\n",
        "# ------------------------------\n",
        "# tomar un ejemplo de test\n",
        "x_test = torch.from_numpy(X_test[0:1]).float().to(DEVICE)  # (1, 365, 1)\n",
        "y_true = Y_test[0:1]\n",
        "with torch.no_grad():\n",
        "    y_pred = model2(x_test, tgt=None, teacher_forcing_ratio=0.0)\n",
        "\n",
        "y_pred = y_pred.cpu().numpy().squeeze()\n",
        "y_true = y_true.squeeze()\n",
        "\n",
        "# des-normalizar: recordamos que normalizamos por la serie de entrenamiento (en el generator usamos su propia mu/sigma)\n",
        "# en este notebook normalizamos por serie individual, por lo que la comparativa se mantiene sin revertir.\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(np.arange(TARGET_LEN), y_true, label='True (30 días)')\n",
        "plt.plot(np.arange(TARGET_LEN), y_pred, label='Predicción (30 días)')\n",
        "plt.legend()\n",
        "plt.title('Predicción de precipitación - 30 días')\n",
        "plt.xlabel('Día (horizonte)')\n",
        "plt.ylabel('Precipitación (norm.)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print('Predicciones (primeros 10):', y_pred[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Notas pedagógicas\n",
        "------------------------------\n",
        "- Aquí usamos un encoder que resume la serie completa en el estado oculto. Para horizontes largos\n",
        "  (365->365) esa aproximación puede fallar: conviene usar attention o transformers.\n",
        "- Utilizamos normalización por serie. En práctica real se normaliza con parámetros del conjunto de entrenamiento.\n",
        "- Teacher forcing acelera el aprendizaje en decoder, pero produce discrepancias en inferencia (exposure bias).\n",
        "- Para datasets reales: limpiar, imputar ceros, transformar (log1p) y evaluar métricas de error (RMSE, MAE)."
      ],
      "metadata": {
        "id": "wOw46OCc0DQ6"
      }
    }
  ]
}