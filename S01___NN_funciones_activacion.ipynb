{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObvWoaFGe/WEgVe5g42PGj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcmachicao/deep_learning_2025_curso/blob/main/NN___funciones_activacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_XTCfecj7q6"
      },
      "outputs": [],
      "source": [
        "# Código pedagógico sobre algebra lineal básica, derivadas/gradientes y funciones de activación.\n",
        "# Ejecuta todo en un cuaderno o script para ver resultados y gráficos.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Vectores, matrices, producto punto\n",
        "# -----------------------------\n",
        "\n",
        "def demo_vectors_matrices():\n",
        "    print(\"=== VECTORES Y MATRICES ===\")\n",
        "    # vectores (1-D arrays)\n",
        "    v = np.array([1.0, 2.0, -1.0])\n",
        "    w = np.array([0.5, -1.0, 2.0])\n",
        "    print(\"v =\", v)\n",
        "    print(\"w =\", w)\n",
        "\n",
        "    # producto punto (dot)\n",
        "    dot_vw = np.dot(v, w)\n",
        "    print(\"Producto punto v · w =\", dot_vw)\n",
        "\n",
        "    # matrices (2-D arrays)\n",
        "    A = np.array([[2.0, 0.0, -1.0],\n",
        "                  [1.0, 3.0,  2.0]])\n",
        "    B = np.array([[1.0, -1.0],\n",
        "                  [0.0,  2.0],\n",
        "                  [3.0,  1.0]])\n",
        "    print(\"A shape:\", A.shape)\n",
        "    print(\"B shape:\", B.shape)\n",
        "\n",
        "    # producto matricial (matmul)\n",
        "    C = A @ B   # equivalente a np.matmul(A, B)\n",
        "    print(\"A @ B =\\n\", C)\n",
        "\n",
        "    # multiplicación elemento a elemento (Hadamard)\n",
        "    # requiere shapes iguales (broadcasting posible)\n",
        "    x = np.array([[1.0, 2.0],\n",
        "                  [3.0, 4.0]])\n",
        "    y = np.array([[2.0, 0.5],\n",
        "                  [0.0, -1.0]])\n",
        "    print(\"Hadamard x * y =\\n\", x * y)\n",
        "\n",
        "    # operaciones útiles\n",
        "    print(\"Transpuesta de A:\\n\", A.T)\n",
        "    print(\"Suma de vectores v + w =\", v + w)\n",
        "\n",
        "demo_vectors_matrices()\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Derivadas y gradientes\n",
        "# -----------------------------\n",
        "\n",
        "# a) Derivada de funciones escalares simples\n",
        "def derivative_scalar(f, x, eps=1e-6):\n",
        "    \"\"\"Aproximación numérica de la derivada usando diferencia central\"\"\"\n",
        "    return (f(x + eps) - f(x - eps)) / (2 * eps)\n",
        "\n",
        "# ejemplo: f(x) = x^2  => f'(x) = 2x\n",
        "def demo_derivative_scalar():\n",
        "    print(\"\\n=== DERIVADAS (ESCALAR) ===\")\n",
        "    f = lambda x: x**2\n",
        "    for x in [0.0, 1.0, -2.0, 3.5]:\n",
        "        num = derivative_scalar(f, x)\n",
        "        exact = 2*x\n",
        "        print(f\"x={x:4.1f} | deriv.num={num: .6f} | deriv.exact={exact: .6f}\")\n",
        "\n",
        "demo_derivative_scalar()\n",
        "\n",
        "# b) Gradiente de funciones vectoriales\n",
        "# Ejemplo: f(x) = x^T A x  (A es matriz n x n)\n",
        "# grad f = (A + A^T) x\n",
        "def grad_quadratic(A, x):\n",
        "    return (A + A.T) @ x\n",
        "\n",
        "def numerical_gradient(f, x, eps=1e-6):\n",
        "    \"\"\"Calcula gradiente numérico para función f: R^n -> R en punto x\"\"\"\n",
        "    x = x.astype(float)\n",
        "    grad = np.zeros_like(x)\n",
        "    for i in range(len(x)):\n",
        "        x_plus = x.copy(); x_minus = x.copy()\n",
        "        x_plus[i] += eps\n",
        "        x_minus[i] -= eps\n",
        "        grad[i] = (f(x_plus) - f(x_minus)) / (2*eps)\n",
        "    return grad\n",
        "\n",
        "def demo_gradients():\n",
        "    print(\"\\n=== GRADIENTES ===\")\n",
        "    A = np.array([[2.0, 1.0, 0.0],\n",
        "                  [1.0, 3.0, 2.0],\n",
        "                  [0.0, 2.0, 4.0]])\n",
        "    x = np.array([1.0, -1.0, 0.5])\n",
        "    f = lambda z: z.T @ A @ z  # escalar\n",
        "    analytic = grad_quadratic(A, x)\n",
        "    numeric = numerical_gradient(f, x)\n",
        "    print(\"x =\", x)\n",
        "    print(\"gradiente analítico =\", analytic)\n",
        "    print(\"gradiente numérico     =\", numeric)\n",
        "    print(\"diferencia (norm) =\", np.linalg.norm(analytic - numeric))\n",
        "\n",
        "demo_gradients()\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Funciones de activación: Sigmoid, ReLU, Tanh (y sus derivadas)\n",
        "# -----------------------------\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid estable numéricamente\"\"\"\n",
        "    x = np.array(x, dtype=float)\n",
        "    # evitar overflow: manejar positivos y negativos\n",
        "    pos = x >= 0\n",
        "    neg = ~pos\n",
        "    out = np.empty_like(x, dtype=float)\n",
        "    out[pos] = 1.0 / (1.0 + np.exp(-x[pos]))\n",
        "    # para valores negativos, usar exp(x) / (1+exp(x))\n",
        "    out[neg] = np.exp(x[neg]) / (1.0 + np.exp(x[neg]))\n",
        "    return out\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def relu(x):\n",
        "    x = np.array(x, dtype=float)\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_prime(x):\n",
        "    x = np.array(x, dtype=float)\n",
        "    # subgradiente: derivada 0 para x<0, 1 para x>0, asignamos 0 en x==0 (es una convención)\n",
        "    grad = np.zeros_like(x)\n",
        "    grad[x > 0] = 1.0\n",
        "    return grad\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_prime(x):\n",
        "    t = np.tanh(x)\n",
        "    return 1 - t**2\n",
        "\n",
        "# pruebas rápidas\n",
        "def demo_activations():\n",
        "    print(\"\\n=== ACTIVACIONES ===\")\n",
        "    xs = np.array([-4.0, -1.0, -0.1, 0.0, 0.2, 1.0, 3.0])\n",
        "    print(\"x:\", xs)\n",
        "    print(\"sigmoid(x):\", sigmoid(xs))\n",
        "    print(\"sigmoid'(x):\", sigmoid_prime(xs))\n",
        "    print(\"ReLU(x):\", relu(xs))\n",
        "    print(\"ReLU'(x):\", relu_prime(xs))\n",
        "    print(\"tanh(x):\", tanh(xs))\n",
        "    print(\"tanh'(x):\", tanh_prime(xs))\n",
        "\n",
        "demo_activations()\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Graficar funciones de activación y derivadas\n",
        "# -----------------------------\n",
        "\n",
        "x_plot = np.linspace(-6, 6, 501)\n",
        "\n",
        "# Sigmoid plot (función y derivada en la misma figura)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(x_plot, sigmoid(x_plot), label=\"sigmoid(x)\")\n",
        "plt.plot(x_plot, sigmoid_prime(x_plot), label=\"sigmoid'(x)\", linestyle='--')\n",
        "plt.title(\"Sigmoid y su derivada\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ReLU plot\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(x_plot, relu(x_plot), label=\"ReLU(x)\")\n",
        "plt.plot(x_plot, relu_prime(x_plot), label=\"ReLU'(x)\", linestyle='--')\n",
        "plt.title(\"ReLU y su derivada (subgradiente)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Tanh plot\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(x_plot, tanh(x_plot), label=\"tanh(x)\")\n",
        "plt.plot(x_plot, tanh_prime(x_plot), label=\"tanh'(x)\", linestyle='--')\n",
        "plt.title(\"Tanh y su derivada\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Mini-ejercicio sugerido para estudiantes\n",
        "# -----------------------------\n",
        "exercise_text = \"\"\"\n",
        "EJERCICIO (sugerido):\n",
        "1) Implementa numéricamente la derivada de sigmoid usando diferencia central y compara con\n",
        "   sigmoid_prime en varios puntos (p.ej. x = -10, -2, -0.5, 0, 0.5, 2, 10).\n",
        "2) Considera la función f(x) = log(1 + exp(w^T x + b)) (softplus de un escalar).\n",
        "   - Implementa f y calcula su gradiente respecto a x analíticamente.\n",
        "   - Comprueba con gradiente numérico.\n",
        "3) Toma un pequeño dataset de imágenes (p.ej. MNIST o un subconjunto) y crea un clasificador\n",
        "   muy simple con una capa lineal + softmax. Entrena pocas épocas y observa pérdida y accuracy.\n",
        "\"\"\"\n",
        "\n",
        "print(exercise_text)\n"
      ]
    }
  ]
}
