{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO2omL5tqDK4d6PFrWuEpV3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"dq6RPNRuHaza"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":29,"metadata":{"id":"tWaMR8NMHQvx","executionInfo":{"status":"ok","timestamp":1761779037660,"user_tz":300,"elapsed":1916,"user":{"displayName":"José Carlos Machicao Valencia","userId":"02338708306941462864"}}},"outputs":[],"source":["from transformers import AutoModel, AutoTokenizer\n","import torch\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n","\n","inputs = tokenizer(\"Deep learning is awesome!\", return_tensors=\"pt\")\n","outputs = model(**inputs, output_attentions=True, output_hidden_states=True) # Added output_attentions and output_hidden_states"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca7738e2","executionInfo":{"status":"ok","timestamp":1761779041770,"user_tz":300,"elapsed":313,"user":{"displayName":"José Carlos Machicao Valencia","userId":"02338708306941462864"}},"outputId":"0fac48a4-e21d-4dbd-8d1d-eaeb8556fd8a"},"source":["# Access the last hidden state from the outputs\n","last_hidden_state = outputs.last_hidden_state\n","print(\"Shape of the last hidden state:\", last_hidden_state.shape)\n","\n","# You can also access other outputs if available, for example:\n","attentions = outputs.attentions\n","hidden_states = outputs.hidden_states\n","print(attentions)"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of the last hidden state: torch.Size([1, 7, 768])\n","(tensor([[[[9.2010e-02, 7.0300e-02, 1.7699e-01, 6.6514e-02, 7.4351e-02,\n","           1.1171e-01, 4.0813e-01],\n","          [2.0479e-01, 4.6417e-02, 1.5348e-01, 7.7433e-02, 1.5218e-01,\n","           2.3405e-01, 1.3165e-01],\n","          [2.7185e-01, 6.2297e-02, 1.1724e-01, 7.6816e-02, 1.4196e-01,\n","           2.2617e-01, 1.0367e-01],\n","          [3.2929e-01, 9.4178e-02, 1.7418e-01, 3.7162e-02, 1.3396e-01,\n","           1.5358e-01, 7.7647e-02],\n","          [2.3691e-01, 7.4713e-02, 8.1912e-02, 9.7331e-02, 1.0050e-01,\n","           1.8106e-01, 2.2758e-01],\n","          [2.5490e-01, 1.1865e-01, 1.0832e-01, 7.6234e-02, 2.1038e-01,\n","           6.6618e-02, 1.6490e-01],\n","          [2.4854e-01, 8.5162e-02, 7.4471e-02, 9.1349e-02, 3.8234e-02,\n","           1.1857e-01, 3.4367e-01]],\n","\n","         [[9.8199e-01, 2.7207e-03, 1.7563e-03, 3.6101e-03, 1.7300e-03,\n","           1.8096e-03, 6.3879e-03],\n","          [1.3182e-02, 6.1160e-02, 2.6447e-01, 2.2376e-02, 3.6175e-01,\n","           6.8858e-02, 2.0820e-01],\n","          [1.1479e-02, 8.5062e-02, 2.6450e-01, 2.2084e-02, 3.5006e-01,\n","           1.0038e-01, 1.6644e-01],\n","          [9.5732e-03, 1.9105e-01, 2.7714e-01, 3.4094e-02, 3.7233e-01,\n","           6.0502e-02, 5.5309e-02],\n","          [2.6395e-03, 2.0912e-01, 1.0802e-01, 2.8731e-02, 2.4274e-01,\n","           1.5119e-01, 2.5756e-01],\n","          [8.6916e-03, 2.0931e-01, 4.4316e-02, 3.4749e-02, 3.1848e-01,\n","           2.5033e-01, 1.3413e-01],\n","          [2.6642e-02, 2.2518e-01, 3.1575e-02, 2.8915e-01, 6.8514e-02,\n","           1.5553e-01, 2.0340e-01]],\n","\n","         [[8.8938e-01, 3.4865e-03, 1.0492e-02, 1.0237e-02, 7.2369e-03,\n","           7.5602e-03, 7.1612e-02],\n","          [9.8776e-01, 5.2114e-04, 8.2609e-05, 1.6205e-03, 2.4235e-04,\n","           9.9872e-04, 8.7727e-03],\n","          [3.6232e-01, 6.1025e-01, 2.8556e-04, 1.5170e-04, 1.0530e-03,\n","           8.3260e-05, 2.5851e-02],\n","          [3.2148e-01, 1.9115e-03, 6.6237e-01, 8.1895e-03, 1.3514e-03,\n","           1.3696e-03, 3.3274e-03],\n","          [8.2787e-01, 4.0425e-05, 9.9341e-04, 1.4213e-01, 1.9138e-03,\n","           3.7384e-04, 2.6678e-02],\n","          [8.8775e-02, 3.6154e-05, 1.3043e-05, 2.7140e-03, 9.0202e-01,\n","           2.0199e-03, 4.4232e-03],\n","          [1.2628e-01, 6.6416e-05, 4.9787e-04, 5.2821e-04, 1.4387e-03,\n","           7.4360e-01, 1.2760e-01]],\n","\n","         [[7.0285e-01, 4.1559e-02, 3.1303e-02, 3.1035e-02, 3.2604e-02,\n","           2.8097e-02, 1.3255e-01],\n","          [8.2392e-01, 2.9410e-02, 9.2066e-03, 3.1114e-03, 2.9481e-03,\n","           1.4304e-03, 1.2997e-01],\n","          [3.7864e-01, 2.2111e-01, 9.8810e-02, 3.0904e-02, 3.0618e-02,\n","           6.6070e-03, 2.3331e-01],\n","          [1.1458e-01, 2.3334e-01, 5.0802e-01, 6.2340e-02, 8.4926e-03,\n","           1.2763e-02, 6.0458e-02],\n","          [2.3176e-01, 3.5476e-02, 1.3363e-01, 4.5361e-01, 6.8145e-02,\n","           1.1386e-02, 6.5995e-02],\n","          [3.7400e-02, 1.7124e-02, 4.5658e-02, 2.7327e-01, 5.2909e-01,\n","           6.8748e-02, 2.8712e-02],\n","          [6.2338e-02, 7.3720e-03, 8.7736e-03, 2.1899e-02, 3.3214e-02,\n","           8.9253e-02, 7.7715e-01]],\n","\n","         [[6.0414e-01, 6.8254e-02, 6.7556e-02, 6.6507e-02, 6.5708e-02,\n","           1.0469e-01, 2.3137e-02],\n","          [4.9056e-01, 7.5073e-03, 1.2066e-01, 8.5023e-02, 7.1642e-02,\n","           1.6485e-01, 5.9756e-02],\n","          [4.6194e-01, 1.6735e-01, 7.9042e-03, 5.6357e-02, 8.6655e-02,\n","           1.5240e-01, 6.7398e-02],\n","          [7.7494e-02, 7.6079e-02, 7.5516e-02, 1.6619e-01, 2.1351e-01,\n","           3.3342e-01, 5.7780e-02],\n","          [7.5296e-01, 4.1851e-02, 5.1842e-02, 7.3900e-02, 4.7105e-03,\n","           6.5094e-02, 9.6413e-03],\n","          [4.6639e-01, 1.3359e-01, 1.6423e-01, 7.8250e-02, 7.2780e-02,\n","           4.8756e-02, 3.5999e-02],\n","          [4.5146e-03, 1.8230e-03, 1.2153e-03, 1.6494e-03, 7.1338e-03,\n","           1.8014e-03, 9.8186e-01]],\n","\n","         [[4.9378e-01, 6.0701e-02, 9.8462e-02, 1.7705e-01, 4.9400e-02,\n","           6.0171e-02, 6.0436e-02],\n","          [1.1134e-01, 1.0496e-01, 2.1862e-01, 7.7291e-02, 1.2883e-01,\n","           5.0131e-02, 3.0882e-01],\n","          [1.6226e-01, 7.7298e-02, 4.9932e-02, 6.9958e-02, 2.6856e-01,\n","           3.7324e-02, 3.3466e-01],\n","          [1.2193e-01, 5.7118e-02, 8.1677e-02, 5.6312e-02, 1.1814e-01,\n","           2.7606e-01, 2.8876e-01],\n","          [5.3810e-01, 4.0498e-02, 6.7356e-02, 3.3890e-02, 7.1436e-02,\n","           4.9977e-02, 1.9874e-01],\n","          [9.7239e-02, 5.3373e-02, 7.0112e-02, 7.0028e-02, 1.2678e-01,\n","           1.8018e-01, 4.0228e-01],\n","          [2.1361e-01, 6.4583e-02, 1.3445e-01, 2.3075e-01, 6.1946e-02,\n","           1.9170e-01, 1.0296e-01]],\n","\n","         [[9.8364e-01, 2.3203e-03, 2.6995e-03, 3.9251e-03, 6.8886e-04,\n","           4.3502e-04, 6.2866e-03],\n","          [3.2190e-01, 1.7850e-01, 1.0266e-01, 5.6986e-02, 1.6333e-01,\n","           9.4689e-02, 8.1939e-02],\n","          [3.2131e-01, 6.0845e-02, 2.5061e-01, 1.3346e-01, 8.4266e-02,\n","           5.9024e-02, 9.0483e-02],\n","          [1.1220e-01, 7.0109e-02, 2.1062e-01, 1.2220e-01, 1.9149e-01,\n","           1.5898e-01, 1.3440e-01],\n","          [1.6344e-01, 7.8325e-02, 1.2678e-01, 7.3919e-02, 3.9849e-01,\n","           1.1610e-01, 4.2948e-02],\n","          [5.7095e-02, 1.3970e-01, 1.6847e-01, 1.1661e-01, 3.0938e-01,\n","           1.6132e-01, 4.7425e-02],\n","          [8.6050e-01, 3.0278e-02, 2.8179e-02, 1.8238e-02, 2.5902e-02,\n","           1.1355e-02, 2.5544e-02]],\n","\n","         [[3.2239e-01, 1.0353e-01, 6.3770e-02, 3.6784e-02, 6.4705e-02,\n","           2.2956e-01, 1.7926e-01],\n","          [6.2749e-01, 3.8738e-02, 8.4380e-02, 3.5945e-02, 9.4131e-02,\n","           3.3699e-02, 8.5617e-02],\n","          [3.0422e-01, 1.1301e-01, 1.3782e-01, 4.2086e-02, 1.7439e-01,\n","           4.5479e-02, 1.8300e-01],\n","          [4.5052e-01, 6.2078e-02, 8.7885e-03, 1.8300e-01, 2.4497e-02,\n","           5.7586e-02, 2.1353e-01],\n","          [3.8237e-01, 1.1136e-01, 6.5140e-02, 3.9754e-02, 1.4793e-01,\n","           6.8832e-02, 1.8461e-01],\n","          [5.4510e-01, 7.3853e-02, 9.6381e-02, 7.6054e-02, 1.0099e-01,\n","           4.6459e-02, 6.1166e-02],\n","          [3.1471e-01, 6.7252e-02, 5.2620e-02, 5.3197e-02, 5.9437e-02,\n","           4.1616e-01, 3.6617e-02]],\n","\n","         [[6.8547e-01, 6.0330e-02, 8.4570e-02, 3.5172e-02, 5.8878e-02,\n","           2.5582e-02, 5.0003e-02],\n","          [5.4720e-02, 2.7637e-02, 3.0903e-01, 2.7766e-01, 2.1700e-01,\n","           7.4864e-02, 3.9081e-02],\n","          [1.9996e-01, 2.4497e-02, 3.0923e-02, 9.8435e-02, 3.3737e-01,\n","           1.7111e-01, 1.3771e-01],\n","          [3.2869e-03, 5.1959e-03, 4.3836e-03, 2.2128e-02, 5.0729e-02,\n","           6.3988e-01, 2.7439e-01],\n","          [2.7214e-01, 1.1898e-02, 2.6627e-02, 9.4293e-03, 6.6399e-02,\n","           2.7147e-01, 3.4204e-01],\n","          [9.5793e-02, 1.1151e-02, 3.4133e-02, 3.0633e-02, 7.9506e-02,\n","           2.4402e-01, 5.0477e-01],\n","          [3.7137e-01, 3.0564e-02, 1.0392e-01, 2.0769e-01, 1.4912e-01,\n","           7.2269e-02, 6.5072e-02]],\n","\n","         [[5.7468e-01, 3.7092e-02, 5.7718e-02, 8.8096e-02, 1.5223e-02,\n","           3.4184e-02, 1.9301e-01],\n","          [3.4570e-01, 5.3637e-01, 1.0250e-02, 1.1801e-02, 2.8948e-02,\n","           2.9250e-03, 6.4011e-02],\n","          [4.3003e-01, 1.5145e-02, 3.7648e-01, 7.9011e-03, 2.7295e-02,\n","           1.5143e-02, 1.2801e-01],\n","          [9.4369e-01, 7.3266e-03, 1.7458e-03, 5.2992e-03, 4.9607e-03,\n","           3.6491e-03, 3.3327e-02],\n","          [5.1045e-01, 1.1884e-02, 1.3356e-02, 4.3414e-03, 3.6047e-01,\n","           5.6166e-02, 4.3339e-02],\n","          [4.9546e-01, 1.0224e-02, 5.7160e-03, 2.0068e-02, 3.7129e-02,\n","           3.8498e-01, 4.6418e-02],\n","          [7.6469e-01, 4.8223e-02, 2.6079e-02, 9.5147e-02, 1.4990e-02,\n","           9.6410e-03, 4.1227e-02]],\n","\n","         [[8.1853e-01, 1.5421e-02, 2.3385e-02, 2.7383e-02, 1.8163e-02,\n","           2.0772e-02, 7.6345e-02],\n","          [7.7604e-02, 1.2120e-03, 9.0440e-01, 1.2257e-02, 1.5288e-04,\n","           7.6716e-04, 3.6052e-03],\n","          [2.5319e-01, 1.1295e-03, 3.1844e-04, 7.1535e-01, 3.6317e-03,\n","           3.8408e-03, 2.2536e-02],\n","          [9.9367e-03, 5.3689e-04, 1.2660e-04, 1.7369e-03, 9.7973e-01,\n","           6.7971e-03, 1.1394e-03],\n","          [1.1461e-01, 1.5835e-04, 1.2069e-03, 9.6397e-04, 4.1498e-03,\n","           8.2871e-01, 5.0210e-02],\n","          [9.1671e-02, 1.7417e-04, 6.8517e-05, 2.0572e-04, 5.4622e-04,\n","           9.5722e-03, 8.9776e-01],\n","          [7.3911e-01, 2.1508e-03, 1.8327e-02, 3.2421e-03, 7.9177e-03,\n","           8.3099e-03, 2.2095e-01]],\n","\n","         [[7.4257e-01, 1.4239e-02, 1.7526e-02, 2.1158e-02, 2.6685e-02,\n","           4.8950e-03, 1.7293e-01],\n","          [6.3950e-01, 9.6245e-03, 4.4133e-02, 5.6461e-02, 9.6419e-02,\n","           2.6205e-02, 1.2766e-01],\n","          [4.6980e-01, 2.3464e-01, 1.1712e-02, 1.6009e-01, 3.1328e-02,\n","           7.8552e-03, 8.4580e-02],\n","          [3.2550e-01, 1.8948e-01, 1.5604e-01, 4.4681e-02, 1.4553e-01,\n","           7.1218e-02, 6.7549e-02],\n","          [7.0207e-01, 1.4237e-02, 1.4427e-02, 3.7181e-02, 2.7242e-02,\n","           9.2526e-03, 1.9559e-01],\n","          [2.2661e-01, 6.0072e-03, 2.8952e-03, 4.2562e-02, 2.0480e-01,\n","           4.1677e-02, 4.7545e-01],\n","          [3.5054e-01, 2.4987e-02, 2.9021e-02, 7.6488e-02, 1.3699e-01,\n","           5.0337e-02, 3.3164e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[9.7618e-01, 5.3613e-03, 4.5134e-03, 1.5362e-03, 3.4785e-03,\n","           2.2351e-03, 6.6954e-03],\n","          [1.2026e-05, 5.6399e-06, 9.9998e-01, 1.5647e-06, 1.6400e-09,\n","           7.6467e-08, 2.4580e-06],\n","          [4.8928e-05, 3.8325e-08, 6.0876e-06, 9.9993e-01, 2.7585e-07,\n","           5.5331e-08, 1.9612e-05],\n","          [5.7839e-07, 2.2540e-07, 6.1438e-08, 3.4321e-07, 1.0000e+00,\n","           8.7447e-08, 4.0752e-08],\n","          [4.5638e-07, 1.5417e-09, 1.4415e-08, 1.4687e-08, 2.2538e-07,\n","           9.9999e-01, 6.6350e-06],\n","          [1.9842e-02, 1.8708e-05, 8.2247e-07, 1.7290e-06, 8.2105e-08,\n","           1.5948e-03, 9.7854e-01],\n","          [9.2929e-01, 3.4933e-03, 2.7767e-02, 1.0059e-03, 9.0717e-04,\n","           4.3550e-04, 3.7097e-02]],\n","\n","         [[5.1049e-01, 2.0139e-02, 3.4621e-02, 4.1122e-02, 2.8282e-02,\n","           5.4538e-02, 3.1081e-01],\n","          [3.4017e-01, 4.0792e-02, 3.5520e-01, 2.7783e-02, 1.5583e-03,\n","           1.9851e-03, 2.3251e-01],\n","          [1.8950e-01, 6.6624e-01, 2.4488e-03, 2.6750e-02, 1.3172e-03,\n","           4.5182e-03, 1.0922e-01],\n","          [4.3956e-01, 5.3732e-02, 1.1204e-01, 3.5357e-02, 6.0483e-02,\n","           4.4741e-02, 2.5408e-01],\n","          [3.6244e-01, 9.7807e-03, 1.2191e-02, 2.8585e-01, 3.4023e-03,\n","           7.6737e-02, 2.4959e-01],\n","          [4.0439e-01, 1.9229e-03, 1.3384e-02, 8.2209e-02, 1.9423e-01,\n","           5.4161e-02, 2.4970e-01],\n","          [6.9938e-01, 4.1735e-03, 5.5908e-03, 1.1349e-02, 1.4658e-02,\n","           3.2937e-02, 2.3191e-01]],\n","\n","         [[6.6634e-01, 6.1041e-03, 6.4570e-03, 1.1319e-02, 5.8401e-03,\n","           1.1810e-02, 2.9213e-01],\n","          [6.5290e-01, 1.2618e-02, 2.0193e-02, 5.3624e-03, 1.5758e-02,\n","           2.0779e-02, 2.7239e-01],\n","          [4.9568e-01, 1.6135e-02, 3.1940e-02, 1.3503e-02, 4.9615e-02,\n","           1.2125e-02, 3.8100e-01],\n","          [8.0925e-01, 2.6852e-03, 3.3552e-03, 1.6290e-03, 1.9498e-03,\n","           9.4142e-03, 1.7171e-01],\n","          [6.2945e-01, 9.6314e-02, 2.3690e-02, 7.2065e-03, 1.3671e-02,\n","           3.7381e-02, 1.9229e-01],\n","          [5.2911e-01, 1.2929e-02, 5.6049e-03, 1.2448e-02, 3.7286e-03,\n","           7.0227e-02, 3.6595e-01],\n","          [4.3036e-01, 4.4505e-02, 4.9826e-02, 2.3207e-02, 3.0936e-02,\n","           4.5251e-02, 3.7591e-01]],\n","\n","         [[6.6900e-01, 3.7420e-02, 1.9098e-02, 1.6797e-02, 3.4774e-02,\n","           5.8740e-02, 1.6417e-01],\n","          [4.8930e-01, 7.2471e-03, 4.5537e-02, 3.4524e-02, 4.3150e-02,\n","           1.1678e-01, 2.6346e-01],\n","          [6.0002e-01, 1.2673e-02, 2.3692e-03, 2.5824e-02, 1.9164e-02,\n","           6.2020e-02, 2.7793e-01],\n","          [3.8101e-01, 2.8190e-02, 7.3044e-02, 2.8326e-02, 3.0054e-02,\n","           1.6892e-01, 2.9046e-01],\n","          [6.6046e-01, 1.3944e-02, 6.3717e-02, 2.2056e-02, 2.4333e-04,\n","           9.1380e-03, 2.3044e-01],\n","          [5.4669e-01, 5.5071e-02, 4.0889e-02, 3.6586e-02, 5.3799e-03,\n","           1.1188e-03, 3.1427e-01],\n","          [8.5613e-01, 1.6467e-02, 7.8535e-03, 8.1408e-03, 1.2854e-02,\n","           1.8239e-02, 8.0318e-02]],\n","\n","         [[9.0371e-01, 1.6792e-02, 1.8220e-02, 1.8162e-02, 1.5984e-02,\n","           1.9554e-02, 7.5772e-03],\n","          [8.1868e-01, 2.0010e-02, 5.1579e-02, 4.1771e-02, 1.8428e-02,\n","           4.2827e-02, 6.7092e-03],\n","          [6.5715e-01, 4.7831e-02, 7.4947e-02, 1.4047e-01, 5.6793e-02,\n","           1.5043e-02, 7.7595e-03],\n","          [6.5943e-01, 4.9961e-02, 6.4883e-02, 9.8439e-02, 3.3440e-02,\n","           8.5159e-02, 8.6870e-03],\n","          [7.5117e-01, 4.8044e-02, 6.8020e-02, 6.6066e-02, 3.8419e-03,\n","           5.2045e-02, 1.0818e-02],\n","          [7.1714e-01, 5.6345e-02, 2.2673e-02, 1.0964e-01, 4.8867e-02,\n","           3.9135e-02, 6.1998e-03],\n","          [6.0062e-03, 4.2869e-04, 4.2250e-04, 3.2864e-05, 4.0775e-04,\n","           9.5655e-05, 9.9261e-01]],\n","\n","         [[4.8201e-01, 5.1652e-02, 8.2499e-02, 7.0157e-02, 6.4859e-02,\n","           1.2514e-01, 1.2368e-01],\n","          [7.4884e-01, 3.5640e-02, 5.5159e-03, 3.4533e-02, 1.6464e-02,\n","           7.1694e-02, 8.7310e-02],\n","          [6.8299e-01, 8.7545e-03, 1.2570e-02, 7.3049e-02, 5.0396e-03,\n","           8.2030e-02, 1.3557e-01],\n","          [3.5130e-01, 1.0800e-02, 4.2139e-02, 1.8720e-01, 4.5282e-02,\n","           1.5351e-01, 2.0977e-01],\n","          [5.8773e-01, 1.6497e-02, 1.1077e-02, 4.4537e-02, 2.4747e-02,\n","           7.4878e-02, 2.4053e-01],\n","          [3.6722e-01, 2.6997e-02, 2.3449e-02, 1.5030e-01, 6.0238e-02,\n","           2.1849e-01, 1.5331e-01],\n","          [5.7357e-01, 1.2348e-02, 1.6200e-02, 1.9378e-02, 1.0932e-02,\n","           1.7676e-02, 3.4990e-01]],\n","\n","         [[1.8315e-01, 4.3910e-02, 4.9102e-02, 1.2517e-01, 7.2126e-02,\n","           3.9897e-02, 4.8664e-01],\n","          [1.0931e-01, 6.9765e-01, 2.4135e-03, 1.7600e-03, 1.2686e-02,\n","           4.2747e-03, 1.7190e-01],\n","          [6.2828e-02, 1.8612e-03, 7.7239e-01, 3.9227e-03, 1.8652e-02,\n","           3.6133e-03, 1.3673e-01],\n","          [1.4440e-01, 2.9608e-03, 1.4380e-03, 5.6895e-01, 1.3862e-03,\n","           6.1409e-03, 2.7473e-01],\n","          [8.7236e-03, 1.4479e-03, 1.7882e-03, 3.0229e-04, 9.6385e-01,\n","           4.9966e-03, 1.8891e-02],\n","          [6.6247e-02, 5.1697e-03, 1.7584e-03, 5.3692e-03, 1.0695e-02,\n","           7.6290e-01, 1.4786e-01],\n","          [1.8230e-01, 4.1945e-02, 7.7892e-02, 1.6641e-01, 1.4781e-01,\n","           6.3113e-02, 3.2054e-01]],\n","\n","         [[6.8444e-01, 2.1066e-02, 2.6789e-02, 2.9826e-02, 1.7659e-02,\n","           6.4773e-02, 1.5544e-01],\n","          [5.5854e-01, 3.4934e-02, 7.0073e-02, 1.9659e-02, 2.0105e-02,\n","           7.8150e-02, 2.1854e-01],\n","          [4.0906e-01, 9.0860e-02, 1.1259e-02, 6.1944e-02, 3.0925e-02,\n","           1.0624e-01, 2.8972e-01],\n","          [3.9379e-01, 4.4939e-02, 4.7414e-02, 6.6506e-02, 3.0924e-02,\n","           2.4336e-01, 1.7306e-01],\n","          [4.7750e-01, 5.8515e-02, 2.3668e-02, 3.4237e-02, 3.9606e-03,\n","           3.2260e-02, 3.6986e-01],\n","          [5.3763e-01, 8.1359e-02, 6.2913e-02, 4.4589e-02, 9.0849e-03,\n","           1.3377e-02, 2.5105e-01],\n","          [9.2379e-01, 5.3844e-03, 8.8967e-03, 6.2593e-03, 2.2813e-03,\n","           9.9021e-03, 4.3481e-02]],\n","\n","         [[9.1584e-01, 4.8842e-03, 1.0705e-02, 1.2127e-02, 9.7109e-03,\n","           4.0483e-02, 6.2479e-03],\n","          [9.1576e-01, 4.4263e-03, 1.8542e-02, 6.2161e-03, 5.7246e-03,\n","           4.7848e-02, 1.4850e-03],\n","          [8.6480e-01, 1.9505e-02, 2.7533e-02, 3.3105e-02, 5.7875e-03,\n","           3.8039e-02, 1.1232e-02],\n","          [3.1935e-01, 8.8041e-02, 1.3885e-01, 4.1372e-02, 3.2250e-02,\n","           3.5060e-01, 2.9544e-02],\n","          [7.0699e-01, 3.5480e-02, 6.8685e-02, 3.6551e-02, 3.1238e-02,\n","           1.0525e-01, 1.5806e-02],\n","          [4.1504e-01, 2.4169e-01, 3.5907e-02, 6.2137e-02, 5.9328e-02,\n","           1.6621e-01, 1.9687e-02],\n","          [4.9915e-03, 4.7541e-04, 3.9691e-04, 2.4736e-03, 1.1812e-03,\n","           1.7741e-03, 9.8871e-01]],\n","\n","         [[6.9986e-01, 1.8754e-02, 4.7883e-03, 2.9078e-03, 1.7730e-03,\n","           1.8528e-03, 2.7007e-01],\n","          [1.3057e-06, 2.3501e-06, 9.9999e-01, 1.4398e-06, 1.1294e-10,\n","           1.2762e-08, 1.9570e-06],\n","          [4.3018e-06, 1.6730e-09, 2.2265e-06, 9.9999e-01, 2.0902e-08,\n","           4.4921e-09, 2.1136e-06],\n","          [3.3018e-07, 6.9469e-08, 7.1913e-10, 5.5451e-07, 1.0000e+00,\n","           1.9963e-08, 7.8134e-08],\n","          [1.7456e-07, 2.5493e-10, 2.6040e-09, 5.0197e-09, 1.2505e-07,\n","           1.0000e+00, 5.4839e-07],\n","          [8.9108e-03, 4.7923e-05, 3.6013e-08, 9.8460e-08, 1.9210e-08,\n","           1.0452e-04, 9.9094e-01],\n","          [9.2661e-01, 6.9702e-03, 2.3963e-02, 6.8927e-03, 1.3546e-03,\n","           8.4748e-03, 2.5739e-02]],\n","\n","         [[7.1892e-01, 3.1472e-02, 2.5348e-02, 1.6015e-02, 3.0133e-02,\n","           4.2846e-02, 1.3526e-01],\n","          [5.4504e-01, 9.7807e-02, 1.2128e-01, 2.2965e-02, 1.1199e-01,\n","           4.4994e-02, 5.5923e-02],\n","          [1.8347e-01, 1.4760e-01, 5.1242e-01, 7.4996e-02, 1.4644e-02,\n","           1.9894e-02, 4.6979e-02],\n","          [7.8472e-01, 1.2884e-02, 3.7948e-02, 4.1386e-02, 2.8488e-02,\n","           5.2566e-02, 4.2010e-02],\n","          [3.5642e-01, 5.5279e-02, 6.4875e-02, 1.3500e-01, 1.8986e-01,\n","           1.2022e-01, 7.8342e-02],\n","          [4.9702e-01, 1.8962e-02, 3.6486e-02, 3.3282e-02, 1.3194e-01,\n","           2.2718e-01, 5.5137e-02],\n","          [9.1978e-01, 5.7505e-03, 1.5150e-03, 7.4939e-03, 4.7341e-03,\n","           9.1468e-03, 5.1584e-02]],\n","\n","         [[6.1745e-01, 7.0902e-03, 2.0740e-02, 3.0147e-02, 8.3817e-03,\n","           2.9127e-02, 2.8707e-01],\n","          [6.2926e-01, 1.9608e-02, 7.4788e-02, 1.3495e-01, 1.5831e-02,\n","           2.8092e-02, 9.7470e-02],\n","          [4.8636e-01, 2.9645e-02, 5.8896e-02, 1.1011e-01, 8.5548e-02,\n","           1.1417e-01, 1.1528e-01],\n","          [2.5582e-01, 1.7384e-02, 8.0209e-02, 3.0593e-02, 1.6162e-01,\n","           2.9654e-01, 1.5783e-01],\n","          [6.0691e-01, 9.9445e-03, 4.0727e-02, 9.8290e-03, 1.7302e-02,\n","           7.4807e-02, 2.4048e-01],\n","          [4.8556e-01, 5.3213e-03, 1.7471e-02, 1.9624e-02, 1.4710e-02,\n","           6.5071e-02, 3.9225e-01],\n","          [8.3033e-01, 3.2756e-03, 7.5822e-03, 1.0241e-02, 4.8734e-03,\n","           1.0706e-02, 1.3299e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[4.8392e-02, 1.3543e-01, 1.3769e-01, 1.7977e-02, 6.9949e-02,\n","           3.7016e-02, 5.5354e-01],\n","          [4.0405e-02, 5.5589e-02, 3.4973e-01, 6.2550e-03, 3.5406e-02,\n","           8.9162e-03, 5.0370e-01],\n","          [2.2327e-02, 1.8999e-01, 3.5007e-02, 2.1045e-02, 3.7600e-02,\n","           2.2015e-03, 6.9183e-01],\n","          [2.5355e-02, 1.4398e-02, 2.0933e-02, 1.2553e-02, 1.1111e-01,\n","           1.0080e-02, 8.0557e-01],\n","          [3.6489e-02, 2.1283e-02, 5.4578e-02, 2.9535e-02, 4.4455e-02,\n","           9.3534e-03, 8.0431e-01],\n","          [2.2210e-02, 1.9936e-02, 3.2310e-02, 2.0962e-02, 8.0613e-02,\n","           5.4583e-02, 7.6939e-01],\n","          [3.0466e-03, 1.4241e-04, 6.6879e-05, 4.1944e-05, 2.6840e-05,\n","           3.2453e-04, 9.9635e-01]],\n","\n","         [[3.3602e-02, 1.9670e-01, 2.4201e-01, 1.6156e-01, 1.4873e-01,\n","           1.7397e-01, 4.3429e-02],\n","          [1.6804e-01, 4.9279e-02, 5.4570e-02, 1.2163e-01, 7.5160e-02,\n","           6.8654e-02, 4.6267e-01],\n","          [7.7984e-02, 5.7278e-02, 6.9767e-02, 8.5836e-02, 5.5212e-02,\n","           1.0432e-01, 5.4960e-01],\n","          [4.3026e-02, 1.3603e-01, 8.3527e-02, 1.3833e-01, 5.8553e-02,\n","           1.4517e-01, 3.9536e-01],\n","          [5.7491e-02, 2.5969e-02, 2.0394e-02, 8.3407e-02, 5.5673e-02,\n","           9.1614e-02, 6.6545e-01],\n","          [2.2950e-02, 2.2067e-01, 2.6128e-01, 1.5427e-01, 1.5607e-01,\n","           4.7528e-02, 1.3723e-01],\n","          [3.3388e-02, 6.0011e-03, 5.0792e-03, 7.7696e-03, 5.6262e-03,\n","           7.8951e-03, 9.3424e-01]],\n","\n","         [[5.4857e-01, 1.5851e-02, 8.0979e-03, 2.1080e-02, 1.7945e-02,\n","           1.2677e-01, 2.6169e-01],\n","          [1.3461e-01, 3.2495e-03, 1.6066e-02, 1.6756e-02, 2.6065e-02,\n","           4.3845e-02, 7.5941e-01],\n","          [7.9822e-02, 1.1879e-02, 5.4017e-03, 8.3159e-03, 6.3035e-03,\n","           1.1422e-02, 8.7686e-01],\n","          [4.3588e-02, 2.4679e-02, 1.4871e-02, 3.7110e-02, 1.0512e-02,\n","           2.0234e-02, 8.4901e-01],\n","          [5.2063e-02, 4.8182e-02, 2.8690e-02, 1.5699e-02, 1.9681e-02,\n","           9.4845e-03, 8.2620e-01],\n","          [8.6682e-02, 5.7264e-02, 5.1294e-02, 6.0726e-02, 3.7141e-02,\n","           3.7379e-02, 6.6951e-01],\n","          [8.4058e-03, 1.4058e-05, 7.2122e-05, 2.0631e-05, 3.1380e-05,\n","           8.5265e-05, 9.9137e-01]],\n","\n","         [[9.8520e-03, 2.4343e-03, 1.9523e-03, 1.0143e-03, 1.6182e-03,\n","           6.5164e-03, 9.7661e-01],\n","          [1.8932e-02, 5.4143e-02, 1.0815e-04, 3.5581e-05, 8.0224e-04,\n","           3.1176e-04, 9.2567e-01],\n","          [1.5064e-02, 7.6755e-05, 1.8640e-02, 2.8217e-04, 2.8870e-04,\n","           3.8517e-04, 9.6526e-01],\n","          [3.2294e-02, 9.0033e-05, 7.8376e-04, 1.0933e-01, 8.8297e-05,\n","           1.8873e-03, 8.5553e-01],\n","          [9.0902e-03, 9.8716e-04, 1.0957e-03, 9.1353e-05, 5.0638e-02,\n","           8.4557e-04, 9.3725e-01],\n","          [1.8662e-02, 3.9659e-04, 1.2910e-03, 5.4246e-04, 7.7583e-04,\n","           2.6552e-01, 7.1281e-01],\n","          [3.9991e-04, 4.9863e-07, 5.9395e-07, 4.3050e-07, 1.1823e-06,\n","           1.6133e-06, 9.9960e-01]],\n","\n","         [[1.0008e-01, 9.0455e-02, 1.3569e-01, 2.6844e-02, 4.0580e-02,\n","           1.2078e-01, 4.8558e-01],\n","          [7.0302e-02, 2.5100e-02, 7.3640e-02, 2.3792e-02, 2.6444e-02,\n","           2.9617e-02, 7.5111e-01],\n","          [2.1000e-02, 4.4206e-02, 1.0345e-01, 2.2213e-02, 3.5412e-02,\n","           1.6563e-02, 7.5716e-01],\n","          [3.0084e-02, 3.1878e-02, 2.9608e-02, 1.8172e-02, 3.0340e-02,\n","           3.2843e-02, 8.2708e-01],\n","          [1.1645e-02, 1.6046e-02, 1.7981e-02, 1.4835e-02, 7.5102e-03,\n","           4.8776e-03, 9.2710e-01],\n","          [4.7674e-02, 7.9768e-02, 5.6115e-02, 3.9049e-02, 5.1442e-02,\n","           2.8728e-02, 6.9722e-01],\n","          [8.9907e-03, 4.8353e-04, 4.3413e-04, 2.0004e-04, 3.5599e-04,\n","           3.5754e-04, 9.8918e-01]],\n","\n","         [[2.2918e-03, 1.4833e-03, 5.5922e-03, 2.3051e-02, 4.6779e-03,\n","           6.1717e-03, 9.5673e-01],\n","          [1.6996e-01, 3.1161e-02, 1.7728e-01, 3.1126e-01, 5.6525e-02,\n","           9.4804e-03, 2.4433e-01],\n","          [1.4959e-01, 6.4193e-03, 5.7215e-03, 1.8973e-01, 8.8937e-02,\n","           1.4097e-01, 4.1863e-01],\n","          [2.2642e-01, 7.7759e-02, 2.7686e-02, 2.2513e-02, 2.8343e-01,\n","           2.2701e-01, 1.3518e-01],\n","          [1.7450e-01, 7.7085e-03, 4.4620e-02, 4.0760e-02, 9.2861e-03,\n","           1.5384e-01, 5.6929e-01],\n","          [5.8023e-02, 1.1413e-03, 2.9001e-02, 7.7746e-02, 1.9452e-02,\n","           5.0401e-02, 7.6423e-01],\n","          [4.8810e-04, 7.9171e-05, 1.7190e-04, 3.5116e-04, 3.2549e-04,\n","           3.8653e-04, 9.9820e-01]],\n","\n","         [[1.4369e-02, 8.0393e-03, 2.4531e-02, 3.6897e-03, 3.3723e-02,\n","           2.7095e-02, 8.8855e-01],\n","          [7.3407e-02, 7.5365e-02, 1.6586e-02, 1.2018e-01, 5.9715e-02,\n","           7.8151e-02, 5.7660e-01],\n","          [5.9369e-02, 3.8403e-01, 1.9044e-02, 7.0531e-02, 1.4815e-01,\n","           2.7877e-02, 2.9100e-01],\n","          [1.3168e-01, 1.4602e-01, 2.8925e-02, 2.3694e-02, 5.2302e-02,\n","           7.9348e-02, 5.3803e-01],\n","          [5.1315e-02, 3.9634e-02, 9.8320e-03, 3.9370e-01, 4.5219e-02,\n","           2.0710e-01, 2.5320e-01],\n","          [8.3042e-02, 1.2824e-02, 7.3423e-03, 3.0543e-03, 7.6691e-03,\n","           5.1555e-02, 8.3451e-01],\n","          [2.2029e-03, 1.2524e-04, 5.4497e-05, 8.9759e-05, 1.1743e-04,\n","           1.0999e-03, 9.9631e-01]],\n","\n","         [[3.8905e-02, 9.6538e-03, 9.4437e-03, 1.1455e-02, 2.2018e-02,\n","           8.6980e-03, 8.9983e-01],\n","          [5.2747e-02, 9.0589e-03, 5.9585e-03, 8.6749e-03, 1.2720e-03,\n","           1.9475e-03, 9.2034e-01],\n","          [3.0567e-02, 5.7447e-01, 5.8358e-03, 1.3739e-02, 1.0414e-03,\n","           3.8210e-03, 3.7052e-01],\n","          [4.1313e-02, 2.0498e-02, 4.1703e-01, 1.9746e-02, 1.8627e-02,\n","           1.0846e-02, 4.7194e-01],\n","          [9.7466e-03, 2.3024e-03, 9.0834e-03, 4.7096e-01, 5.8498e-03,\n","           2.6462e-03, 4.9941e-01],\n","          [1.9778e-02, 2.5153e-03, 1.7327e-02, 3.1608e-02, 3.0752e-01,\n","           4.9081e-02, 5.7217e-01],\n","          [2.9169e-03, 1.2665e-03, 2.8655e-04, 1.0710e-03, 1.5575e-03,\n","           5.6167e-04, 9.9234e-01]],\n","\n","         [[5.9205e-02, 2.9493e-02, 2.6896e-02, 1.3187e-02, 8.1449e-03,\n","           3.7467e-02, 8.2561e-01],\n","          [5.9449e-02, 1.0784e-02, 2.9398e-02, 7.5823e-02, 6.4248e-02,\n","           1.3475e-01, 6.2555e-01],\n","          [4.2455e-02, 8.1431e-03, 1.2064e-02, 6.6881e-02, 5.9006e-02,\n","           6.0233e-02, 7.5122e-01],\n","          [5.3788e-02, 1.0695e-02, 1.7621e-02, 6.0414e-02, 7.8468e-02,\n","           1.5126e-01, 6.2776e-01],\n","          [2.6788e-02, 4.5131e-03, 4.3124e-03, 1.7350e-02, 4.0861e-02,\n","           4.0584e-02, 8.6559e-01],\n","          [5.7547e-02, 1.2129e-02, 1.6018e-02, 1.5358e-02, 3.7539e-02,\n","           5.1295e-02, 8.1011e-01],\n","          [1.3609e-02, 2.8269e-03, 1.6384e-03, 1.5840e-03, 3.2843e-03,\n","           1.9179e-03, 9.7514e-01]],\n","\n","         [[5.1504e-02, 5.4054e-02, 7.3797e-02, 1.4887e-02, 5.0240e-02,\n","           1.9269e-02, 7.3625e-01],\n","          [1.0268e-01, 1.2100e-02, 3.0171e-02, 4.7311e-02, 1.2534e-01,\n","           3.9543e-02, 6.4285e-01],\n","          [1.3738e-01, 2.3251e-02, 6.3371e-03, 5.7439e-02, 4.4692e-02,\n","           9.0910e-02, 6.3999e-01],\n","          [9.3200e-02, 2.0858e-02, 7.6248e-02, 1.4723e-01, 5.5510e-02,\n","           9.6306e-02, 5.1065e-01],\n","          [7.9307e-02, 9.8645e-02, 5.4348e-02, 6.0748e-02, 3.8123e-02,\n","           4.1280e-02, 6.2755e-01],\n","          [1.1233e-01, 2.8699e-02, 5.3326e-02, 5.9502e-02, 1.4596e-02,\n","           1.6369e-02, 7.1518e-01],\n","          [2.4009e-02, 1.2064e-02, 4.7913e-03, 6.3507e-03, 8.1013e-03,\n","           8.1120e-03, 9.3657e-01]],\n","\n","         [[5.7224e-02, 1.3300e-02, 1.0437e-02, 6.1198e-03, 2.8627e-03,\n","           1.2645e-02, 8.9741e-01],\n","          [1.4337e-01, 2.2490e-02, 9.0696e-03, 8.3617e-03, 2.4441e-03,\n","           5.3032e-03, 8.0896e-01],\n","          [8.5399e-02, 1.7525e-01, 3.4604e-02, 1.8617e-02, 2.7531e-03,\n","           1.1449e-02, 6.7193e-01],\n","          [3.8829e-02, 5.7048e-01, 2.2815e-01, 2.5341e-02, 4.1693e-03,\n","           5.7240e-03, 1.2730e-01],\n","          [7.5872e-02, 1.7844e-01, 2.0087e-01, 1.8660e-01, 3.5863e-02,\n","           1.6313e-02, 3.0605e-01],\n","          [8.0112e-02, 8.6656e-02, 9.8282e-02, 2.5348e-01, 1.1500e-01,\n","           4.0313e-02, 3.2615e-01],\n","          [1.5586e-02, 8.8862e-04, 4.1192e-04, 9.3276e-04, 2.5740e-04,\n","           8.9799e-04, 9.8103e-01]],\n","\n","         [[1.0031e-01, 1.0858e-01, 2.9442e-02, 2.3003e-02, 1.0886e-02,\n","           9.1789e-03, 7.1860e-01],\n","          [4.6473e-02, 5.9266e-03, 7.3721e-01, 1.2029e-02, 1.9684e-03,\n","           3.1187e-03, 1.9327e-01],\n","          [7.8628e-02, 3.8055e-03, 1.0150e-02, 3.6749e-01, 1.0093e-02,\n","           3.5855e-03, 5.2625e-01],\n","          [5.3744e-02, 3.6326e-03, 1.5342e-02, 1.6483e-02, 6.0237e-01,\n","           1.7855e-02, 2.9058e-01],\n","          [5.3752e-02, 1.2384e-03, 2.0798e-03, 1.5794e-02, 1.5258e-02,\n","           6.8092e-02, 8.4379e-01],\n","          [5.2233e-02, 5.0836e-02, 5.8190e-03, 1.7566e-02, 6.5392e-03,\n","           9.3395e-03, 8.5767e-01],\n","          [1.0703e-02, 3.5788e-03, 2.2220e-03, 6.0058e-03, 2.5189e-03,\n","           1.5792e-03, 9.7339e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[1.4314e-02, 4.1357e-02, 1.3147e-02, 8.7859e-03, 1.1096e-02,\n","           8.8191e-02, 8.2311e-01],\n","          [6.0858e-02, 3.4529e-02, 2.4403e-02, 2.1872e-02, 1.7424e-02,\n","           3.7911e-02, 8.0300e-01],\n","          [8.3822e-02, 4.0508e-02, 2.8928e-02, 3.3457e-02, 1.5651e-02,\n","           4.4938e-02, 7.5270e-01],\n","          [5.0543e-02, 2.0525e-01, 7.6509e-02, 3.6953e-02, 1.8734e-02,\n","           4.6963e-02, 5.6505e-01],\n","          [7.3794e-02, 1.3182e-01, 1.6874e-01, 6.6669e-02, 3.8897e-02,\n","           5.3288e-02, 4.6680e-01],\n","          [1.6532e-01, 5.5456e-02, 5.1592e-02, 6.2441e-02, 6.2351e-02,\n","           1.0764e-01, 4.9520e-01],\n","          [6.4251e-03, 2.4612e-03, 1.6931e-03, 3.8587e-03, 4.3484e-03,\n","           2.2038e-02, 9.5918e-01]],\n","\n","         [[5.3551e-02, 5.0281e-02, 3.3736e-02, 4.1036e-02, 6.3563e-02,\n","           1.9385e-01, 5.6398e-01],\n","          [3.5655e-02, 1.6051e-02, 3.8026e-02, 4.6479e-02, 3.6454e-02,\n","           2.2137e-02, 8.0520e-01],\n","          [1.9162e-02, 6.6411e-03, 8.5676e-03, 3.4568e-02, 3.9358e-02,\n","           1.4940e-02, 8.7676e-01],\n","          [1.2214e-02, 2.7192e-02, 2.9090e-02, 4.6514e-02, 2.3749e-01,\n","           2.2746e-02, 6.2475e-01],\n","          [6.3359e-03, 5.9670e-03, 8.3586e-03, 1.1944e-02, 1.3286e-02,\n","           1.7102e-02, 9.3701e-01],\n","          [1.7435e-02, 1.7615e-02, 1.9321e-02, 2.8601e-02, 5.9865e-02,\n","           3.2607e-02, 8.2456e-01],\n","          [9.5160e-03, 1.8453e-03, 2.4160e-03, 3.4840e-03, 2.9515e-03,\n","           3.5183e-03, 9.7627e-01]],\n","\n","         [[3.0178e-03, 2.1058e-02, 7.2605e-03, 7.5437e-03, 1.9481e-03,\n","           1.5562e-03, 9.5762e-01],\n","          [3.2160e-02, 9.8674e-03, 2.3596e-01, 4.5361e-02, 9.6000e-03,\n","           4.2728e-03, 6.6278e-01],\n","          [2.1445e-02, 4.6052e-03, 9.6889e-03, 2.5737e-01, 1.6674e-02,\n","           1.2363e-02, 6.7785e-01],\n","          [4.1167e-02, 8.3223e-03, 1.5440e-02, 2.4056e-02, 3.4146e-01,\n","           6.1356e-02, 5.0820e-01],\n","          [4.7879e-02, 5.1653e-04, 2.8888e-03, 1.3847e-02, 7.3141e-03,\n","           1.0929e-01, 8.1826e-01],\n","          [6.6092e-03, 7.8862e-03, 1.3739e-03, 3.1317e-03, 2.7285e-03,\n","           4.3780e-03, 9.7389e-01],\n","          [2.2286e-02, 2.3608e-03, 1.9706e-03, 2.8922e-03, 1.9476e-03,\n","           6.9255e-03, 9.6162e-01]],\n","\n","         [[1.6527e-02, 1.8369e-02, 1.1010e-02, 1.1625e-02, 2.5087e-02,\n","           6.2590e-02, 8.5479e-01],\n","          [8.0113e-03, 2.1435e-02, 1.5503e-02, 2.1696e-02, 3.6217e-02,\n","           5.8808e-03, 8.9126e-01],\n","          [8.4841e-03, 1.2345e-01, 1.1272e-02, 2.6900e-02, 2.6107e-02,\n","           2.3524e-03, 8.0143e-01],\n","          [1.3012e-02, 2.1435e-02, 6.9023e-03, 3.3984e-02, 7.3681e-02,\n","           1.4542e-02, 8.3644e-01],\n","          [1.0594e-02, 1.1577e-02, 1.1558e-02, 2.7337e-02, 3.7459e-02,\n","           2.3466e-02, 8.7801e-01],\n","          [2.5317e-02, 4.9045e-03, 8.7963e-03, 2.3312e-02, 3.5024e-02,\n","           2.8299e-02, 8.7435e-01],\n","          [1.2827e-02, 2.9144e-03, 1.6572e-03, 2.0489e-03, 1.9652e-03,\n","           7.0170e-03, 9.7157e-01]],\n","\n","         [[2.2537e-03, 5.3556e-04, 7.9094e-04, 2.0777e-03, 2.1992e-03,\n","           2.9985e-03, 9.8914e-01],\n","          [4.5690e-01, 5.9222e-03, 4.9003e-04, 5.7328e-03, 2.6579e-04,\n","           1.9409e-02, 5.1128e-01],\n","          [5.1104e-02, 8.0832e-01, 4.3289e-03, 1.1001e-02, 8.7655e-04,\n","           5.2407e-04, 1.2384e-01],\n","          [5.1886e-03, 6.7817e-03, 5.7885e-01, 1.1467e-02, 3.3266e-02,\n","           9.0263e-03, 3.5542e-01],\n","          [2.9352e-03, 4.7817e-04, 3.8141e-03, 8.6259e-01, 2.5659e-03,\n","           1.2424e-03, 1.2638e-01],\n","          [1.0390e-03, 2.3255e-04, 9.3499e-03, 2.2231e-02, 2.5725e-01,\n","           8.1062e-03, 7.0179e-01],\n","          [9.3827e-03, 7.3587e-04, 3.5416e-04, 1.0642e-03, 4.9783e-04,\n","           9.3112e-04, 9.8703e-01]],\n","\n","         [[1.4695e-02, 1.2132e-01, 1.3909e-01, 1.6782e-01, 9.2885e-02,\n","           1.0010e-01, 3.6408e-01],\n","          [5.2926e-02, 5.7984e-02, 2.0853e-01, 4.1190e-02, 5.0436e-02,\n","           3.7657e-02, 5.5127e-01],\n","          [5.7772e-02, 1.7580e-01, 1.3360e-01, 4.7547e-02, 4.5269e-02,\n","           2.1073e-02, 5.1894e-01],\n","          [4.8446e-02, 1.8415e-01, 1.6816e-01, 1.1032e-01, 1.7150e-01,\n","           3.5182e-02, 2.8223e-01],\n","          [2.9632e-02, 9.1073e-02, 1.2495e-01, 8.7245e-02, 5.2276e-02,\n","           2.2894e-02, 5.9193e-01],\n","          [1.7908e-02, 5.3403e-02, 4.2272e-02, 7.9621e-02, 1.0105e-01,\n","           5.4281e-02, 6.5147e-01],\n","          [8.4622e-04, 1.4726e-03, 1.6379e-03, 6.9149e-04, 1.6210e-03,\n","           1.2225e-03, 9.9251e-01]],\n","\n","         [[7.0102e-02, 5.8316e-03, 4.9575e-03, 3.3811e-03, 5.2951e-03,\n","           7.5726e-03, 9.0286e-01],\n","          [3.9016e-03, 1.4494e-03, 4.0678e-02, 2.2685e-03, 6.2623e-03,\n","           3.6191e-03, 9.4182e-01],\n","          [5.1639e-03, 1.1563e-03, 3.2135e-03, 5.4840e-03, 5.6048e-03,\n","           2.4430e-03, 9.7693e-01],\n","          [3.2857e-03, 1.8973e-03, 4.1297e-03, 7.1479e-03, 9.8133e-02,\n","           7.6777e-03, 8.7773e-01],\n","          [1.1798e-02, 1.9986e-03, 3.4114e-03, 1.9347e-03, 6.9132e-03,\n","           1.6070e-02, 9.5787e-01],\n","          [1.3005e-02, 3.3421e-03, 8.3639e-04, 2.5698e-03, 2.8465e-03,\n","           3.5896e-03, 9.7381e-01],\n","          [8.3757e-03, 6.8842e-03, 4.5080e-03, 9.2883e-04, 3.6279e-03,\n","           1.9976e-03, 9.7368e-01]],\n","\n","         [[2.4828e-02, 3.0013e-03, 4.7230e-03, 1.0951e-03, 8.8669e-03,\n","           1.5704e-01, 8.0044e-01],\n","          [1.1096e-02, 1.8441e-03, 1.1261e-03, 7.5965e-03, 2.3299e-04,\n","           2.2328e-03, 9.7587e-01],\n","          [6.1321e-03, 4.4440e-02, 5.3375e-03, 2.5345e-02, 2.8028e-03,\n","           2.9274e-03, 9.1302e-01],\n","          [5.9872e-04, 1.2888e-03, 6.0371e-03, 9.3329e-03, 2.0260e-03,\n","           1.8606e-03, 9.7886e-01],\n","          [2.9086e-03, 1.2150e-03, 1.7330e-03, 1.1383e-01, 2.3181e-03,\n","           1.4222e-03, 8.7657e-01],\n","          [5.7014e-04, 2.2518e-04, 1.6431e-03, 1.2093e-02, 1.6574e-02,\n","           2.1495e-02, 9.4740e-01],\n","          [5.0317e-03, 3.3657e-03, 5.0687e-03, 4.9558e-03, 3.8919e-03,\n","           1.3515e-02, 9.6417e-01]],\n","\n","         [[3.4646e-02, 4.0475e-02, 2.7208e-02, 7.3107e-03, 3.1430e-02,\n","           1.1078e-01, 7.4815e-01],\n","          [8.0201e-02, 2.6902e-02, 5.9721e-02, 3.0923e-01, 1.3665e-01,\n","           1.1468e-01, 2.7262e-01],\n","          [9.4134e-02, 3.2278e-02, 3.6703e-02, 2.9144e-01, 1.4891e-01,\n","           1.1786e-01, 2.7867e-01],\n","          [6.5372e-02, 1.2079e-02, 8.3848e-03, 5.0844e-02, 2.2818e-02,\n","           3.8588e-01, 4.5463e-01],\n","          [4.0039e-02, 1.0545e-02, 7.5775e-03, 1.0759e-02, 1.1587e-02,\n","           1.3699e-01, 7.8250e-01],\n","          [4.8935e-02, 1.1128e-02, 4.2046e-03, 4.9953e-03, 7.9608e-03,\n","           8.1043e-02, 8.4173e-01],\n","          [2.1255e-02, 2.2108e-03, 3.7166e-03, 5.2396e-03, 6.3891e-03,\n","           3.0017e-02, 9.3117e-01]],\n","\n","         [[2.8367e-02, 5.3805e-03, 6.4407e-03, 8.7104e-03, 1.3781e-02,\n","           2.9445e-02, 9.0787e-01],\n","          [9.3780e-02, 8.7437e-03, 8.9118e-03, 1.9103e-02, 1.2040e-02,\n","           1.1236e-02, 8.4619e-01],\n","          [6.7118e-02, 7.6293e-02, 3.1120e-02, 3.2394e-02, 1.6382e-02,\n","           1.8267e-02, 7.5843e-01],\n","          [5.3552e-02, 6.9526e-02, 2.6547e-02, 4.6266e-02, 2.3133e-02,\n","           2.7411e-02, 7.5356e-01],\n","          [1.1317e-02, 1.9639e-02, 4.8159e-02, 2.6131e-01, 8.3026e-02,\n","           2.8091e-02, 5.4846e-01],\n","          [6.3473e-02, 5.5094e-03, 8.9357e-03, 2.7151e-02, 2.8739e-02,\n","           4.3899e-02, 8.2229e-01],\n","          [1.5236e-02, 5.4900e-03, 4.2111e-03, 2.8699e-03, 3.9631e-03,\n","           1.3994e-02, 9.5424e-01]],\n","\n","         [[1.3370e-02, 1.4350e-01, 1.3605e-01, 1.1150e-02, 1.0238e-02,\n","           9.9448e-03, 6.7575e-01],\n","          [4.9882e-02, 4.8661e-02, 5.1976e-01, 2.2233e-02, 1.7196e-02,\n","           8.7716e-03, 3.3350e-01],\n","          [1.7705e-02, 6.3889e-02, 5.0667e-02, 1.5259e-02, 2.0293e-02,\n","           1.2787e-02, 8.1940e-01],\n","          [1.1346e-02, 7.7450e-02, 2.1532e-01, 3.7691e-02, 4.5510e-01,\n","           2.9891e-02, 1.7320e-01],\n","          [1.4187e-02, 5.0365e-02, 1.2502e-01, 6.9486e-02, 2.7919e-02,\n","           1.4562e-02, 6.9846e-01],\n","          [2.0853e-02, 2.8244e-02, 2.9081e-02, 8.4953e-02, 5.4499e-02,\n","           7.1622e-02, 7.1075e-01],\n","          [6.0237e-03, 3.6375e-03, 3.7926e-03, 4.6410e-04, 9.0988e-04,\n","           1.3370e-03, 9.8384e-01]],\n","\n","         [[7.6367e-02, 6.8740e-03, 1.0100e-02, 1.0475e-02, 1.2043e-02,\n","           4.1868e-01, 4.6546e-01],\n","          [1.6367e-02, 1.4129e-02, 5.9435e-03, 7.9413e-03, 6.4136e-04,\n","           4.0993e-04, 9.5457e-01],\n","          [1.2565e-02, 1.8341e-01, 1.2436e-02, 7.7889e-03, 2.1717e-03,\n","           9.8072e-04, 7.8065e-01],\n","          [2.3849e-02, 7.7546e-02, 1.7145e-01, 4.0779e-02, 1.9461e-02,\n","           1.5277e-03, 6.6539e-01],\n","          [1.0934e-02, 1.2863e-02, 4.3351e-02, 1.0629e-01, 1.0928e-02,\n","           4.8068e-03, 8.1083e-01],\n","          [1.3418e-02, 2.3770e-03, 4.0453e-03, 2.4407e-02, 1.2960e-01,\n","           7.0197e-02, 7.5596e-01],\n","          [8.1506e-03, 1.7470e-03, 2.6131e-03, 2.1619e-03, 2.7929e-03,\n","           1.1390e-02, 9.7114e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[8.7948e-03, 4.4750e-02, 3.8944e-01, 1.1302e-02, 3.1652e-02,\n","           9.4969e-02, 4.1909e-01],\n","          [5.4656e-03, 2.4379e-01, 1.4735e-01, 3.1547e-03, 2.7828e-02,\n","           1.2471e-02, 5.5994e-01],\n","          [3.1736e-03, 5.1861e-02, 2.6468e-01, 7.9911e-04, 1.2875e-02,\n","           3.8847e-03, 6.6272e-01],\n","          [5.4944e-03, 2.1485e-02, 7.7898e-02, 7.6080e-02, 2.6140e-01,\n","           3.0461e-02, 5.2718e-01],\n","          [6.4244e-03, 3.8837e-02, 3.5953e-02, 8.8085e-03, 2.5879e-01,\n","           1.4090e-02, 6.3709e-01],\n","          [7.6102e-03, 7.1714e-03, 1.3078e-02, 1.5319e-02, 6.9745e-02,\n","           2.7786e-01, 6.0922e-01],\n","          [2.9647e-04, 4.1590e-03, 2.2436e-02, 1.5875e-03, 4.9879e-03,\n","           3.1607e-03, 9.6337e-01]],\n","\n","         [[1.0663e-02, 3.7523e-02, 5.7704e-02, 1.2636e-02, 4.7062e-03,\n","           1.0527e-02, 8.6624e-01],\n","          [1.2251e-02, 2.5595e-02, 1.1705e-01, 4.1888e-03, 7.0915e-03,\n","           4.0652e-03, 8.2976e-01],\n","          [1.0289e-02, 8.9183e-03, 2.6671e-02, 1.6950e-03, 1.4458e-03,\n","           1.7648e-03, 9.4922e-01],\n","          [4.6261e-02, 2.0350e-01, 2.6707e-01, 4.9243e-02, 3.0352e-02,\n","           1.1511e-02, 3.9207e-01],\n","          [3.4040e-02, 1.1050e-01, 1.3251e-01, 3.2218e-02, 1.4330e-02,\n","           8.3902e-03, 6.6801e-01],\n","          [2.1396e-01, 2.1049e-02, 1.5805e-02, 6.0772e-02, 1.9561e-02,\n","           3.9824e-02, 6.2903e-01],\n","          [7.5414e-03, 1.8169e-02, 3.9187e-02, 7.2401e-03, 5.1396e-03,\n","           4.2664e-03, 9.1846e-01]],\n","\n","         [[1.6846e-03, 8.6068e-03, 3.3428e-03, 4.2089e-02, 4.0408e-02,\n","           2.5075e-02, 8.7879e-01],\n","          [2.3723e-02, 2.8349e-02, 1.9298e-02, 3.7601e-02, 7.8060e-02,\n","           4.7650e-02, 7.6532e-01],\n","          [3.9371e-02, 4.5619e-02, 3.1040e-02, 3.0618e-02, 6.0926e-02,\n","           5.7929e-02, 7.3450e-01],\n","          [6.3362e-02, 4.9462e-02, 2.7907e-02, 1.7898e-01, 1.2717e-01,\n","           1.0166e-01, 4.5146e-01],\n","          [5.7140e-02, 5.3744e-02, 2.8939e-02, 1.4599e-01, 8.8751e-02,\n","           5.2726e-02, 5.7271e-01],\n","          [6.6515e-02, 2.2840e-02, 1.5942e-02, 8.5528e-02, 8.5330e-02,\n","           7.6134e-02, 6.4771e-01],\n","          [7.7455e-03, 1.3272e-02, 6.9641e-03, 1.0751e-02, 9.3049e-03,\n","           8.0170e-03, 9.4394e-01]],\n","\n","         [[1.8874e-02, 7.4449e-03, 7.3096e-03, 2.3914e-02, 5.0583e-02,\n","           4.6699e-02, 8.4518e-01],\n","          [1.0118e-01, 5.8814e-02, 7.8027e-02, 1.8087e-01, 9.8784e-02,\n","           4.3481e-02, 4.3885e-01],\n","          [1.0881e-01, 9.5924e-02, 2.7820e-02, 1.2677e-01, 9.3123e-02,\n","           5.0521e-02, 4.9702e-01],\n","          [6.7839e-02, 1.0184e-02, 3.4031e-03, 2.5588e-02, 6.7633e-02,\n","           1.1258e-01, 7.1277e-01],\n","          [4.1284e-02, 1.3784e-02, 7.0184e-03, 1.5680e-01, 3.1207e-02,\n","           9.3698e-02, 6.5621e-01],\n","          [2.8394e-02, 5.2739e-03, 4.0662e-03, 1.8105e-02, 2.8025e-02,\n","           4.7526e-02, 8.6861e-01],\n","          [1.3430e-01, 2.6624e-02, 3.0599e-02, 4.8185e-02, 3.6663e-02,\n","           1.2940e-01, 5.9423e-01]],\n","\n","         [[5.9128e-03, 7.2210e-03, 8.7607e-03, 6.9934e-03, 4.0607e-03,\n","           1.0045e-01, 8.6660e-01],\n","          [6.7007e-03, 2.1931e-02, 1.5709e-01, 2.1281e-02, 2.4474e-02,\n","           3.2518e-02, 7.3600e-01],\n","          [5.8521e-03, 1.0245e-01, 3.0203e-02, 1.3590e-02, 7.9971e-03,\n","           1.7189e-02, 8.2272e-01],\n","          [1.8054e-02, 1.6098e-02, 2.0207e-02, 4.4656e-02, 5.8130e-02,\n","           1.0676e-01, 7.3610e-01],\n","          [9.9658e-03, 9.1676e-03, 1.0560e-02, 2.8012e-02, 8.8559e-03,\n","           4.8606e-02, 8.8483e-01],\n","          [3.3375e-02, 2.5684e-02, 1.5285e-02, 1.6887e-02, 1.3989e-02,\n","           3.7366e-02, 8.5742e-01],\n","          [2.6610e-02, 8.9139e-03, 1.7306e-02, 1.0311e-02, 1.1813e-02,\n","           7.9795e-02, 8.4525e-01]],\n","\n","         [[1.1721e-02, 1.1030e-01, 1.2415e-01, 1.0114e-01, 7.7495e-02,\n","           4.3256e-02, 5.3194e-01],\n","          [3.4692e-02, 5.6582e-02, 8.3963e-02, 2.0728e-02, 4.9498e-02,\n","           2.3719e-02, 7.3082e-01],\n","          [4.5118e-02, 9.3490e-02, 1.2184e-01, 1.3531e-02, 2.2575e-02,\n","           1.5967e-02, 6.8748e-01],\n","          [9.8072e-02, 9.6785e-02, 1.2223e-01, 1.3374e-01, 1.7733e-01,\n","           3.6286e-02, 3.3555e-01],\n","          [5.0637e-02, 9.7596e-02, 7.3875e-02, 5.1958e-02, 7.4516e-02,\n","           3.3447e-02, 6.1797e-01],\n","          [2.4012e-02, 4.2274e-02, 1.7615e-02, 3.7022e-02, 6.5215e-02,\n","           4.0404e-02, 7.7346e-01],\n","          [5.1198e-02, 8.5083e-02, 9.5012e-02, 6.2018e-02, 7.8235e-02,\n","           1.4115e-01, 4.8730e-01]],\n","\n","         [[1.9434e-02, 8.4672e-03, 8.3310e-03, 6.9501e-03, 5.4915e-03,\n","           1.6185e-02, 9.3514e-01],\n","          [1.1166e-01, 1.5074e-01, 2.0508e-02, 1.3168e-02, 2.7042e-02,\n","           1.7157e-02, 6.5973e-01],\n","          [3.0993e-02, 1.2190e-02, 4.4118e-02, 7.9521e-03, 9.0051e-03,\n","           5.9470e-03, 8.8980e-01],\n","          [6.8009e-02, 3.4612e-03, 2.3508e-03, 7.0942e-02, 7.5500e-03,\n","           6.0317e-03, 8.4166e-01],\n","          [2.9202e-02, 4.1597e-03, 3.7028e-03, 9.2718e-03, 4.2428e-02,\n","           6.8003e-03, 9.0444e-01],\n","          [5.3954e-02, 1.7281e-03, 3.0740e-03, 5.7602e-03, 4.0922e-03,\n","           2.0943e-02, 9.1045e-01],\n","          [2.3497e-02, 7.3407e-03, 9.4419e-03, 4.0885e-03, 3.9439e-03,\n","           8.1133e-03, 9.4357e-01]],\n","\n","         [[9.2492e-03, 2.7061e-02, 1.2379e-01, 2.3598e-02, 2.8869e-02,\n","           3.2938e-02, 7.5449e-01],\n","          [9.6140e-03, 7.2261e-02, 4.3540e-01, 1.2067e-02, 2.2867e-02,\n","           7.9718e-03, 4.3981e-01],\n","          [5.0627e-03, 8.1847e-02, 2.6488e-01, 1.0388e-02, 1.2822e-02,\n","           4.3754e-03, 6.2062e-01],\n","          [6.3718e-03, 7.3391e-02, 9.9858e-02, 1.1225e-01, 2.4268e-01,\n","           2.1672e-02, 4.4378e-01],\n","          [1.5773e-02, 1.5847e-01, 1.3852e-01, 3.9873e-02, 4.2961e-02,\n","           1.4862e-02, 5.8954e-01],\n","          [3.2497e-03, 3.4283e-02, 1.1378e-02, 1.5337e-01, 1.7993e-01,\n","           1.2200e-01, 4.9579e-01],\n","          [9.6163e-04, 1.0915e-02, 3.1349e-02, 5.2287e-03, 1.4018e-02,\n","           7.2804e-03, 9.3025e-01]],\n","\n","         [[6.1812e-02, 2.8712e-02, 5.8132e-02, 4.1221e-02, 7.7074e-02,\n","           1.5417e-01, 5.7888e-01],\n","          [3.0094e-02, 1.3690e-01, 1.4461e-01, 2.5351e-02, 3.7749e-02,\n","           1.5825e-02, 6.0946e-01],\n","          [5.6393e-02, 1.1467e-01, 3.4823e-01, 2.0479e-02, 1.5459e-02,\n","           2.4570e-03, 4.4231e-01],\n","          [5.4536e-02, 3.5919e-02, 4.9163e-02, 1.1168e-01, 6.2871e-02,\n","           2.1725e-02, 6.6410e-01],\n","          [4.5572e-02, 6.1964e-02, 6.0024e-02, 1.3389e-01, 1.1543e-01,\n","           2.3090e-02, 5.6003e-01],\n","          [3.4662e-02, 3.5940e-03, 1.5861e-03, 3.9580e-02, 2.7433e-02,\n","           3.4936e-01, 5.4379e-01],\n","          [2.0887e-01, 5.0386e-02, 5.2186e-02, 8.9032e-02, 7.6211e-02,\n","           1.9597e-01, 3.2735e-01]],\n","\n","         [[3.2048e-02, 1.3548e-02, 1.8728e-02, 2.6060e-02, 2.2014e-02,\n","           2.2480e-02, 8.6512e-01],\n","          [1.9075e-02, 7.8742e-03, 1.0819e-02, 1.5806e-02, 2.6394e-03,\n","           2.0301e-02, 9.2349e-01],\n","          [7.4788e-03, 2.3839e-01, 9.4785e-03, 4.1563e-03, 1.9637e-03,\n","           3.9414e-03, 7.3459e-01],\n","          [4.3352e-03, 2.0156e-03, 1.3479e-02, 1.3252e-02, 9.6348e-03,\n","           1.1136e-02, 9.4615e-01],\n","          [1.1066e-02, 9.2321e-04, 3.0695e-03, 1.0493e-01, 1.6418e-03,\n","           1.9701e-02, 8.5866e-01],\n","          [1.1789e-02, 1.2859e-02, 1.9505e-02, 1.6750e-02, 3.2465e-02,\n","           3.0800e-02, 8.7583e-01],\n","          [1.1724e-02, 1.0393e-02, 2.4942e-02, 1.7942e-02, 1.3957e-02,\n","           1.2310e-02, 9.0873e-01]],\n","\n","         [[1.2295e-02, 2.6983e-01, 7.1399e-02, 6.3753e-02, 2.1616e-01,\n","           6.4510e-02, 3.0205e-01],\n","          [2.2830e-02, 6.7350e-02, 9.7369e-02, 1.1144e-01, 1.2732e-01,\n","           6.0659e-02, 5.1304e-01],\n","          [2.6026e-02, 4.1390e-02, 2.3097e-02, 4.9256e-02, 5.0555e-02,\n","           2.7851e-02, 7.8183e-01],\n","          [5.5833e-02, 1.3136e-01, 7.5581e-02, 6.7460e-02, 2.8995e-01,\n","           7.2808e-02, 3.0701e-01],\n","          [7.8629e-02, 1.4865e-01, 7.7415e-02, 3.1429e-02, 5.9159e-02,\n","           6.4317e-02, 5.4040e-01],\n","          [9.3167e-02, 9.1330e-02, 3.6898e-02, 2.9762e-02, 5.6828e-02,\n","           3.2404e-02, 6.5961e-01],\n","          [2.6409e-02, 1.1257e-01, 5.0589e-02, 3.6084e-02, 1.0773e-01,\n","           6.6450e-02, 6.0016e-01]],\n","\n","         [[5.4789e-02, 7.2696e-02, 1.3455e-01, 1.0063e-01, 5.7584e-02,\n","           7.9042e-02, 5.0071e-01],\n","          [3.9613e-02, 2.6388e-02, 4.8604e-02, 5.8829e-02, 7.1375e-02,\n","           8.3892e-02, 6.7130e-01],\n","          [9.2613e-02, 3.7020e-02, 2.0608e-02, 3.3392e-02, 2.7313e-02,\n","           3.0614e-02, 7.5844e-01],\n","          [9.4213e-02, 6.9603e-02, 7.1103e-02, 6.6039e-02, 3.5777e-02,\n","           8.0021e-02, 5.8324e-01],\n","          [4.5440e-02, 6.1363e-02, 4.1024e-02, 4.1299e-02, 3.8069e-03,\n","           7.4802e-03, 7.9959e-01],\n","          [3.7862e-02, 4.4911e-02, 3.2970e-02, 4.8974e-02, 1.1709e-02,\n","           2.4211e-03, 8.2115e-01],\n","          [1.6797e-01, 6.2517e-02, 6.9103e-02, 9.7041e-02, 7.0365e-02,\n","           1.7033e-01, 3.6267e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[2.0804e-01, 7.6322e-02, 1.2106e-01, 1.0934e-01, 2.6956e-02,\n","           1.3515e-01, 3.2314e-01],\n","          [1.1046e-02, 1.0765e-02, 3.7054e-02, 2.7793e-03, 1.7556e-03,\n","           2.9594e-03, 9.3364e-01],\n","          [3.0067e-02, 1.8228e-02, 4.5984e-02, 9.0322e-03, 4.1880e-03,\n","           9.1309e-03, 8.8337e-01],\n","          [2.1082e-02, 2.9490e-02, 6.3603e-02, 1.6797e-02, 1.2543e-02,\n","           1.7960e-02, 8.3853e-01],\n","          [2.3796e-02, 4.8578e-02, 7.2765e-02, 1.9662e-02, 1.0701e-02,\n","           1.5014e-02, 8.0948e-01],\n","          [5.8095e-02, 3.1877e-02, 4.1950e-02, 6.3626e-02, 3.4051e-02,\n","           6.1389e-02, 7.0901e-01],\n","          [8.7688e-03, 5.7265e-03, 1.1819e-02, 4.5695e-03, 2.9654e-03,\n","           1.0063e-02, 9.5609e-01]],\n","\n","         [[1.2027e-01, 1.9482e-02, 2.5259e-02, 4.7076e-02, 2.4951e-02,\n","           1.7683e-01, 5.8613e-01],\n","          [7.3190e-03, 6.6937e-03, 1.3494e-02, 7.7405e-03, 6.0380e-03,\n","           1.1772e-02, 9.4694e-01],\n","          [4.6697e-02, 5.9883e-02, 5.0875e-02, 5.3094e-02, 3.0704e-02,\n","           3.7620e-02, 7.2113e-01],\n","          [5.9017e-02, 2.2186e-02, 1.3417e-02, 2.9821e-02, 2.7231e-02,\n","           4.2900e-02, 8.0543e-01],\n","          [2.7195e-02, 1.2266e-02, 1.0672e-02, 5.1960e-02, 1.8244e-02,\n","           4.5896e-02, 8.3377e-01],\n","          [7.4385e-02, 2.5004e-02, 1.6016e-02, 8.1790e-02, 3.3651e-02,\n","           1.4060e-01, 6.2856e-01],\n","          [3.5422e-03, 9.0982e-03, 5.7784e-03, 1.3005e-02, 6.5785e-03,\n","           2.4245e-02, 9.3775e-01]],\n","\n","         [[7.9189e-02, 2.8382e-02, 2.9045e-02, 8.4147e-02, 5.0239e-02,\n","           2.7732e-01, 4.5168e-01],\n","          [4.6707e-02, 7.1180e-02, 7.2857e-02, 4.7744e-02, 2.5220e-02,\n","           3.7526e-02, 6.9877e-01],\n","          [6.9410e-02, 4.6235e-02, 7.4974e-02, 3.4159e-02, 2.1508e-02,\n","           4.3878e-02, 7.0984e-01],\n","          [1.0544e-01, 5.0272e-02, 6.9430e-02, 8.9604e-02, 6.0314e-02,\n","           9.0029e-02, 5.3491e-01],\n","          [4.3800e-02, 2.3448e-02, 2.6313e-02, 5.3470e-02, 3.1380e-02,\n","           9.0143e-02, 7.3145e-01],\n","          [5.7858e-02, 1.5005e-02, 2.3463e-02, 6.5057e-02, 4.6364e-02,\n","           3.3187e-01, 4.6038e-01],\n","          [3.1388e-02, 2.2561e-02, 2.9554e-02, 2.9461e-02, 2.0214e-02,\n","           5.2394e-02, 8.1443e-01]],\n","\n","         [[1.3958e-01, 1.2434e-02, 3.0188e-02, 1.4720e-01, 5.4675e-02,\n","           3.7812e-01, 2.3780e-01],\n","          [3.3714e-02, 2.4986e-02, 3.0483e-02, 2.1382e-02, 1.0322e-02,\n","           1.0780e-02, 8.6833e-01],\n","          [3.1951e-02, 2.0845e-02, 3.7365e-02, 1.2793e-02, 8.8443e-03,\n","           7.9517e-03, 8.8025e-01],\n","          [3.2141e-02, 1.8880e-02, 2.5824e-02, 2.4986e-02, 2.1629e-02,\n","           2.3537e-02, 8.5300e-01],\n","          [1.8207e-02, 1.4054e-02, 1.9368e-02, 1.5344e-02, 1.9070e-02,\n","           1.9463e-02, 8.9449e-01],\n","          [1.5077e-02, 6.3830e-03, 8.0051e-03, 7.5432e-03, 1.2308e-02,\n","           2.8309e-02, 9.2238e-01],\n","          [3.5380e-02, 2.7249e-02, 1.9486e-02, 1.4284e-02, 2.3857e-02,\n","           3.8990e-02, 8.4076e-01]],\n","\n","         [[2.6449e-04, 2.3202e-02, 1.2931e-02, 1.3844e-01, 8.4029e-02,\n","           5.9736e-01, 1.4377e-01],\n","          [1.3630e-03, 4.8522e-02, 5.8153e-02, 5.2004e-02, 5.5097e-02,\n","           6.6578e-02, 7.1828e-01],\n","          [8.0631e-04, 7.4994e-02, 7.0526e-02, 9.3046e-02, 9.0506e-02,\n","           7.9183e-02, 5.9094e-01],\n","          [1.7433e-03, 1.0408e-02, 1.1880e-02, 7.2694e-02, 4.2438e-02,\n","           7.6242e-02, 7.8459e-01],\n","          [2.4671e-03, 2.3127e-02, 1.5378e-02, 9.9702e-02, 7.9354e-02,\n","           1.3139e-01, 6.4859e-01],\n","          [8.5593e-04, 1.2470e-02, 8.9134e-03, 1.1010e-01, 4.8989e-02,\n","           1.3035e-01, 6.8832e-01],\n","          [1.4054e-01, 3.1445e-03, 4.4880e-03, 5.3627e-03, 5.6380e-03,\n","           2.1516e-02, 8.1931e-01]],\n","\n","         [[5.9401e-02, 7.4397e-02, 7.4704e-02, 5.7230e-02, 7.6210e-02,\n","           1.5730e-01, 5.0076e-01],\n","          [1.6024e-02, 1.4489e-01, 6.4457e-02, 6.0822e-03, 1.5271e-02,\n","           3.7493e-03, 7.4953e-01],\n","          [4.4267e-02, 5.0349e-02, 8.0257e-02, 3.9252e-03, 6.6788e-03,\n","           4.5567e-03, 8.0997e-01],\n","          [3.2398e-02, 3.8269e-02, 3.3334e-02, 3.4321e-02, 2.1957e-02,\n","           1.7216e-02, 8.2250e-01],\n","          [2.1733e-02, 5.6715e-02, 3.2135e-02, 3.9566e-02, 5.6143e-02,\n","           1.3151e-02, 7.8056e-01],\n","          [6.7561e-02, 1.4206e-02, 1.0854e-02, 2.5650e-02, 1.6259e-02,\n","           8.6076e-02, 7.7939e-01],\n","          [2.7887e-02, 2.4712e-02, 2.8903e-02, 1.5438e-02, 1.8814e-02,\n","           2.9040e-02, 8.5521e-01]],\n","\n","         [[1.8664e-01, 2.3138e-02, 3.4327e-02, 8.6503e-02, 3.7445e-02,\n","           3.1153e-01, 3.2042e-01],\n","          [6.5474e-02, 1.3600e-02, 3.7676e-02, 2.1960e-02, 2.1958e-02,\n","           5.4225e-02, 7.8511e-01],\n","          [1.2766e-01, 5.3624e-02, 8.5601e-02, 7.6299e-02, 6.0974e-02,\n","           8.0895e-02, 5.1495e-01],\n","          [5.8192e-02, 2.4039e-02, 2.5245e-02, 3.4913e-02, 2.7595e-02,\n","           9.9798e-02, 7.3022e-01],\n","          [7.0868e-02, 2.7273e-02, 1.8021e-02, 3.7191e-02, 4.7583e-02,\n","           7.0828e-02, 7.2824e-01],\n","          [5.0966e-02, 1.5550e-02, 1.0038e-02, 2.6919e-02, 1.8530e-02,\n","           1.1478e-01, 7.6321e-01],\n","          [2.3946e-02, 1.8853e-02, 1.7364e-02, 1.9034e-02, 2.4774e-02,\n","           6.8501e-02, 8.2753e-01]],\n","\n","         [[1.8486e-02, 1.5355e-02, 2.5968e-02, 1.6969e-02, 8.4213e-03,\n","           1.7853e-01, 7.3627e-01],\n","          [1.2213e-02, 2.3151e-02, 3.0473e-02, 2.7800e-03, 5.8098e-03,\n","           1.1015e-02, 9.1456e-01],\n","          [2.4567e-02, 2.9871e-02, 6.1452e-02, 9.5143e-03, 9.9538e-03,\n","           2.1180e-02, 8.4346e-01],\n","          [1.4761e-02, 1.1816e-02, 2.4015e-02, 5.4056e-03, 7.5091e-03,\n","           2.9691e-02, 9.0680e-01],\n","          [1.1956e-02, 2.4055e-02, 2.5031e-02, 5.6777e-03, 1.5153e-02,\n","           1.7941e-02, 9.0019e-01],\n","          [1.8517e-02, 9.2094e-03, 1.8024e-02, 1.1634e-02, 1.0182e-02,\n","           5.5892e-02, 8.7654e-01],\n","          [4.0013e-03, 2.6982e-03, 7.6699e-03, 9.3755e-04, 1.5066e-03,\n","           4.2329e-03, 9.7895e-01]],\n","\n","         [[3.0799e-02, 6.2413e-02, 3.7999e-02, 6.8921e-02, 7.2474e-02,\n","           1.2100e-01, 6.0639e-01],\n","          [3.9399e-02, 3.6790e-01, 3.6654e-02, 2.0330e-02, 3.1919e-02,\n","           1.4706e-02, 4.8910e-01],\n","          [9.2047e-02, 5.7689e-02, 1.6238e-01, 2.8549e-02, 1.2953e-02,\n","           2.9567e-02, 6.1681e-01],\n","          [2.1818e-01, 3.2201e-02, 2.5206e-02, 3.1965e-01, 2.1803e-02,\n","           1.1543e-01, 2.6753e-01],\n","          [4.7329e-02, 7.4119e-02, 2.4530e-02, 4.4040e-02, 2.5896e-01,\n","           6.2614e-02, 4.8841e-01],\n","          [8.0839e-02, 1.1647e-02, 1.6218e-02, 6.6442e-02, 3.1452e-02,\n","           3.1359e-01, 4.7982e-01],\n","          [1.0396e-02, 2.0887e-02, 2.0179e-02, 8.6851e-03, 7.9610e-03,\n","           2.2989e-02, 9.0890e-01]],\n","\n","         [[2.2837e-02, 3.8841e-02, 7.8576e-02, 4.7523e-02, 3.8528e-01,\n","           1.7896e-01, 2.4798e-01],\n","          [3.1418e-02, 7.0139e-02, 3.9162e-02, 3.2510e-02, 4.2972e-02,\n","           3.2059e-02, 7.5174e-01],\n","          [4.2166e-02, 3.4561e-02, 1.4466e-01, 5.3019e-02, 3.3189e-02,\n","           3.4962e-02, 6.5744e-01],\n","          [1.4056e-01, 1.0646e-01, 9.6345e-02, 1.6083e-01, 8.0887e-02,\n","           6.0908e-02, 3.5400e-01],\n","          [4.3848e-02, 6.5261e-02, 3.8212e-02, 5.0184e-02, 1.2032e-01,\n","           4.2232e-02, 6.3994e-01],\n","          [8.9054e-02, 3.6033e-02, 3.4214e-02, 5.7874e-02, 8.7115e-02,\n","           7.3187e-02, 6.2252e-01],\n","          [5.2151e-02, 2.9203e-02, 4.6987e-02, 2.4860e-02, 2.6484e-02,\n","           6.6967e-02, 7.5335e-01]],\n","\n","         [[3.8971e-02, 4.4534e-02, 3.1932e-02, 9.2814e-02, 6.3902e-02,\n","           4.5750e-01, 2.7035e-01],\n","          [1.1881e-02, 2.5897e-02, 3.5459e-02, 2.5548e-02, 1.9599e-02,\n","           9.1954e-03, 8.7242e-01],\n","          [1.8204e-02, 2.8370e-02, 4.2063e-02, 2.3189e-02, 1.3523e-02,\n","           1.2464e-02, 8.6219e-01],\n","          [4.4911e-02, 3.1845e-02, 3.2107e-02, 1.0647e-01, 6.7071e-02,\n","           3.1570e-02, 6.8602e-01],\n","          [2.4881e-02, 2.3554e-02, 1.6244e-02, 5.7989e-02, 4.4182e-02,\n","           3.2350e-02, 8.0080e-01],\n","          [4.4111e-02, 2.7611e-02, 2.5529e-02, 6.4014e-02, 4.2305e-02,\n","           2.5046e-02, 7.7138e-01],\n","          [6.2616e-03, 5.5894e-03, 6.5393e-03, 9.1057e-03, 5.9107e-03,\n","           4.4481e-03, 9.6215e-01]],\n","\n","         [[3.2769e-02, 2.6799e-02, 3.7800e-02, 4.2782e-02, 1.9785e-02,\n","           5.9401e-02, 7.8066e-01],\n","          [1.2977e-02, 3.1290e-02, 3.3835e-02, 3.5365e-03, 4.6106e-03,\n","           4.1299e-03, 9.0962e-01],\n","          [3.8350e-02, 3.7266e-02, 6.3621e-02, 7.0205e-03, 5.3358e-03,\n","           7.1305e-03, 8.4128e-01],\n","          [2.5368e-02, 1.9906e-02, 2.1550e-02, 1.5971e-02, 1.0896e-02,\n","           1.4759e-02, 8.9155e-01],\n","          [1.0272e-02, 1.7338e-02, 1.1028e-02, 1.0430e-02, 9.2237e-03,\n","           1.1606e-02, 9.3010e-01],\n","          [2.4788e-02, 1.4119e-02, 1.1370e-02, 1.8734e-02, 9.5216e-03,\n","           4.5851e-02, 8.7562e-01],\n","          [7.3297e-03, 1.2566e-02, 1.6920e-02, 4.8108e-03, 6.0609e-03,\n","           6.4049e-03, 9.4591e-01]]]], grad_fn=<SoftmaxBackward0>))\n"]}]},{"cell_type":"markdown","metadata":{"id":"848b515a"},"source":["# Task\n","Explain how to fine-tune a DistilBERT model for a text classification task and provide a code example."]},{"cell_type":"markdown","metadata":{"id":"5f3d16b0"},"source":["## Prepare a dataset\n","\n","### Subtask:\n","Load or create a small dataset for a text classification task. This dataset should contain text examples and their corresponding labels.\n"]},{"cell_type":"markdown","metadata":{"id":"4c4d5dc2"},"source":["**Reasoning**:\n","Creating lists of texts and labels for a small text classification dataset.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"52a21707","executionInfo":{"status":"ok","timestamp":1761779060293,"user_tz":300,"elapsed":10,"user":{"displayName":"José Carlos Machicao Valencia","userId":"02338708306941462864"}},"outputId":"690d29f9-ff13-4680-c3e9-4031366b496f"},"source":["texts = [\n","    \"This is a great movie!\",\n","    \"I hated that book, it was terrible.\",\n","    \"The weather is nice today.\",\n","    \"Politics are complicated.\",\n","    \"This product is amazing, I love it!\",\n","    \"What a terrible experience.\",\n","    \"The news is depressing.\",\n","    \"Sports events are exciting.\",\n","    \"I feel so happy right now.\",\n","    \"This is the worst thing ever.\"\n","]\n","\n","labels = [\n","    \"positive\",\n","    \"negative\",\n","    \"neutral\",\n","    \"neutral\",\n","    \"positive\",\n","    \"negative\",\n","    \"neutral\",\n","    \"neutral\",\n","    \"positive\",\n","    \"negative\"\n","]\n","\n","print(\"Number of texts:\", len(texts))\n","print(\"Number of labels:\", len(labels))"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of texts: 10\n","Number of labels: 10\n"]}]},{"cell_type":"markdown","metadata":{"id":"25af35b5"},"source":["## Load the pre-trained model and tokenizer\n","\n","### Subtask:\n","Load the DistilBERT model and tokenizer, similar to what you've done before.\n"]},{"cell_type":"markdown","metadata":{"id":"646ee25a"},"source":["**Reasoning**:\n","Import the necessary classes and instantiate the tokenizer and model for sequence classification.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5c4b0f01","executionInfo":{"status":"ok","timestamp":1761779065390,"user_tz":300,"elapsed":1145,"user":{"displayName":"José Carlos Machicao Valencia","userId":"02338708306941462864"}},"outputId":"8a8460fd-254c-475d-94ec-8ab177cb22a1"},"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(set(labels)))\n","\n","print(\"Tokenizer and model loaded successfully.\")"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizer and model loaded successfully.\n"]}]},{"cell_type":"markdown","metadata":{"id":"5480567f"},"source":["## Prepare the data for training\n","\n","### Subtask:\n","Tokenize the text data and format it into a suitable input format for the model (e.g., PyTorch tensors). Split the data into training and validation sets.\n"]},{"cell_type":"markdown","metadata":{"id":"9e3ba6fe"},"source":["**Reasoning**:\n","Create a dictionary for label mapping, tokenize the text data, convert labels to tensors, and split the data into training and validation sets using scikit-learn.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6c4132e3"},"source":["**Reasoning**:\n","Correct the variable name in the print statement to `input_ids_val` and re-run the code to display the shapes of the split data.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zv6YNSdQMXqE","executionInfo":{"status":"ok","timestamp":1761779072140,"user_tz":300,"elapsed":23,"user":{"displayName":"José Carlos Machicao Valencia","userId":"02338708306941462864"}},"outputId":"8c3537d4-6808-4c5d-e5d1-f1ca2c437143"},"source":["from sklearn.model_selection import train_test_split\n","\n","# 1. Create a dictionary mapping labels to integers\n","unique_labels = list(set(labels))\n","label_map = {label: i for i, label in enumerate(unique_labels)}\n","numerical_labels = [label_map[label] for label in labels]\n","\n","# 2. Tokenize the texts\n","tokenized_inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n","\n","# 3. Convert numerical labels to a PyTorch tensor\n","labels_tensor = torch.tensor(numerical_labels)\n","\n","# 4. Split data into training and validation sets\n","input_ids_train, input_ids_val, attention_mask_train, attention_mask_val, labels_train, labels_val = train_test_split(\n","    tokenized_inputs[\"input_ids\"],\n","    tokenized_inputs[\"attention_mask\"],\n","    labels_tensor,\n","    test_size=0.2,\n","    random_state=42  # for reproducibility\n",")\n","\n","print(\"Training input IDs shape:\", input_ids_train.shape)\n","print(\"Validation input IDs shape:\", input_ids_val.shape)\n","print(\"Training attention mask shape:\", attention_mask_train.shape)\n","print(\"Validation attention mask shape:\", attention_mask_val.shape)\n","print(\"Training labels shape:\", labels_train.shape)\n","print(\"Validation labels shape:\", labels_val.shape)"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Training input IDs shape: torch.Size([8, 11])\n","Validation input IDs shape: torch.Size([2, 11])\n","Training attention mask shape: torch.Size([8, 11])\n","Validation attention mask shape: torch.Size([2, 11])\n","Training labels shape: torch.Size([8])\n","Validation labels shape: torch.Size([2])\n"]}]},{"cell_type":"markdown","metadata":{"id":"21f11a06"},"source":["## Define training parameters\n","\n","### Subtask:\n","Set up training parameters such as the number of epochs, learning rate, batch size, etc.\n"]},{"cell_type":"markdown","metadata":{"id":"d87ea0e1"},"source":["**Reasoning**:\n","Define the training parameters as instructed in the subtask.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ae7ea54","executionInfo":{"status":"ok","timestamp":1761779128348,"user_tz":300,"elapsed":45,"user":{"displayName":"José Carlos Machicao Valencia","userId":"02338708306941462864"}},"outputId":"d082f131-f4c7-4751-b564-fb45f85eebad"},"source":["# 1. Define the number of training epochs\n","num_epochs = 50\n","\n","# 2. Define the learning rate for the optimizer\n","learning_rate = 5e-5\n","\n","# 3. Define the batch size for training and evaluation\n","batch_size = 16\n","\n","# 4. Define other relevant parameters (optional)\n","# For this simple example, we'll keep it basic.\n","# You could add parameters like weight_decay, warmup_steps, etc.\n","\n","print(f\"Number of epochs: {num_epochs}\")\n","print(f\"Learning rate: {learning_rate}\")\n","print(f\"Batch size: {batch_size}\")"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of epochs: 50\n","Learning rate: 5e-05\n","Batch size: 16\n"]}]},{"cell_type":"markdown","metadata":{"id":"b87a9007"},"source":["## Define the classification head\n","\n","### Subtask:\n","Add a classification layer on top of the pre-trained model. This layer will take the model's output (e.g., the hidden state of the CLS token) and predict the class label.\n"]},{"cell_type":"markdown","metadata":{"id":"b3fcac31"},"source":["## Define the training loop\n","\n","### Subtask:\n","Implement a training loop that iterates over the training data, calculates the loss, performs backpropagation, and updates the model's weights.\n"]},{"cell_type":"markdown","metadata":{"id":"0eb0b814"},"source":["**Reasoning**:\n","Implement the training loop as described in the instructions, including creating the DataLoader, optimizer, and iterating through epochs and batches to perform the forward pass, calculate loss, and perform backpropagation.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7594852d","executionInfo":{"status":"ok","timestamp":1761779198095,"user_tz":300,"elapsed":66947,"user":{"displayName":"José Carlos Machicao Valencia","userId":"02338708306941462864"}},"outputId":"ef8184d8-5948-40ff-953a-7e4a56cf79af"},"source":["import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","# Create a TensorDataset\n","train_dataset = TensorDataset(input_ids_train, attention_mask_train, labels_train)\n","\n","# Create a DataLoader\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Instantiate an optimizer\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","# Set the model to training mode\n","model.train()\n","\n","# Check for GPU availability and move model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        # Move batch to device\n","        batch = [r.to(device) for r in batch]\n","        input_ids, attention_mask, labels = batch\n","\n","        # Forward pass\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    # Print average loss for the epoch\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch + 1}/{num_epochs}, Average training loss: {avg_train_loss}\")\n","\n","print(\"Training complete.\")"],"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50, Average training loss: 0.6761602163314819\n","Epoch 2/50, Average training loss: 0.573887050151825\n","Epoch 3/50, Average training loss: 0.46184229850769043\n","Epoch 4/50, Average training loss: 0.35568684339523315\n","Epoch 5/50, Average training loss: 0.30437955260276794\n","Epoch 6/50, Average training loss: 0.2529052793979645\n","Epoch 7/50, Average training loss: 0.19536809623241425\n","Epoch 8/50, Average training loss: 0.1546444594860077\n","Epoch 9/50, Average training loss: 0.13495655357837677\n","Epoch 10/50, Average training loss: 0.12131339311599731\n","Epoch 11/50, Average training loss: 0.10749314725399017\n","Epoch 12/50, Average training loss: 0.081902414560318\n","Epoch 13/50, Average training loss: 0.06726101785898209\n","Epoch 14/50, Average training loss: 0.06400955468416214\n","Epoch 15/50, Average training loss: 0.051929574459791183\n","Epoch 16/50, Average training loss: 0.044834237545728683\n","Epoch 17/50, Average training loss: 0.04621502012014389\n","Epoch 18/50, Average training loss: 0.04123428836464882\n","Epoch 19/50, Average training loss: 0.03625139594078064\n","Epoch 20/50, Average training loss: 0.0315546914935112\n","Epoch 21/50, Average training loss: 0.027723997831344604\n","Epoch 22/50, Average training loss: 0.02335137501358986\n","Epoch 23/50, Average training loss: 0.027185767889022827\n","Epoch 24/50, Average training loss: 0.021268330514431\n","Epoch 25/50, Average training loss: 0.023077933117747307\n","Epoch 26/50, Average training loss: 0.01877659000456333\n","Epoch 27/50, Average training loss: 0.021023834124207497\n","Epoch 28/50, Average training loss: 0.0177734587341547\n","Epoch 29/50, Average training loss: 0.014344894327223301\n","Epoch 30/50, Average training loss: 0.014987535774707794\n","Epoch 31/50, Average training loss: 0.013458020985126495\n","Epoch 32/50, Average training loss: 0.013941485434770584\n","Epoch 33/50, Average training loss: 0.012933547608554363\n","Epoch 34/50, Average training loss: 0.012608644552528858\n","Epoch 35/50, Average training loss: 0.013286203145980835\n","Epoch 36/50, Average training loss: 0.010425790213048458\n","Epoch 37/50, Average training loss: 0.010969442315399647\n","Epoch 38/50, Average training loss: 0.01074505690485239\n","Epoch 39/50, Average training loss: 0.010018694214522839\n","Epoch 40/50, Average training loss: 0.009569252841174603\n","Epoch 41/50, Average training loss: 0.009927400387823582\n","Epoch 42/50, Average training loss: 0.006983205676078796\n","Epoch 43/50, Average training loss: 0.008284838870167732\n","Epoch 44/50, Average training loss: 0.008406839333474636\n","Epoch 45/50, Average training loss: 0.007021848578006029\n","Epoch 46/50, Average training loss: 0.007064193021506071\n","Epoch 47/50, Average training loss: 0.007682932540774345\n","Epoch 48/50, Average training loss: 0.0069768400862813\n","Epoch 49/50, Average training loss: 0.006249194499105215\n","Epoch 50/50, Average training loss: 0.006507200188934803\n","Training complete.\n"]}]},{"cell_type":"markdown","metadata":{"id":"0d19c71a"},"source":["## Evaluate the fine-tuned model\n","\n","### Subtask:\n","Evaluate the performance of the fine-tuned model on the validation set using appropriate metrics (e.g., accuracy, precision, recall).\n"]},{"cell_type":"markdown","metadata":{"id":"fe8df255"},"source":["**Reasoning**:\n","Evaluate the fine-tuned model on the validation set using accuracy, precision, recall, and F1-score.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"db00541c","executionInfo":{"status":"ok","timestamp":1761780096724,"user_tz":300,"elapsed":105,"user":{"displayName":"José Carlos Machicao Valencia","userId":"02338708306941462864"}},"outputId":"f87c31b7-8f15-4795-ebab-a9f76d132ac6"},"source":["from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","import numpy as np\n","\n","# 1. Create a TensorDataset for the validation data\n","val_dataset = TensorDataset(input_ids_val, attention_mask_val, labels_val)\n","\n","# 2. Create a DataLoader for the validation dataset\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) # Do not shuffle validation data\n","\n","# 3. Set the model to evaluation mode\n","model.eval()\n","\n","# 4. Initialize lists to store predicted labels and true labels\n","predicted_labels = []\n","true_labels = []\n","\n","# 5. Iterate through the validation DataLoader\n","for batch in val_dataloader:\n","    # Move batch to device\n","    batch = [r.to(device) for r in batch]\n","    input_ids, attention_mask, labels = batch\n","\n","    # 6. Inside the loop, use a torch.no_grad() context\n","    with torch.no_grad():\n","        # 7. Perform a forward pass through the model\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","\n","    # 8. Get the predicted logits from the model outputs\n","    logits = outputs.logits\n","\n","    # 9. Determine the predicted class for each example\n","    predictions = torch.argmax(logits, dim=-1)\n","\n","    # 10. Append the predicted labels and true labels\n","    predicted_labels.extend(predictions.cpu().numpy())\n","    true_labels.extend(labels.cpu().numpy())\n","\n","# 11. After iterating, concatenate the predicted and true labels\n","# (Already done by extending lists and converting to numpy arrays)\n","\n","# Convert lists to numpy arrays for metric calculations\n","predicted_labels = np.array(predicted_labels)\n","true_labels = np.array(true_labels)\n","\n","# 12. Calculate the accuracy of the model\n","accuracy = accuracy_score(true_labels, predicted_labels)\n","\n","# 13. Calculate precision, recall, and F1-score\n","# We use zero_division=0 to avoid warnings/errors for classes with no true/predicted samples\n","precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted', zero_division=0)\n","\n","# 14. Print the calculated evaluation metrics\n","print(\"\\nEvaluation Metrics:\")\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"Precision (weighted): {precision:.4f}\")\n","print(f\"Recall (weighted): {recall:.4f}\")\n","print(f\"F1-score (weighted): {f1_score:.4f}\")"],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluation Metrics:\n","Accuracy: 0.5000\n","Precision (weighted): 0.2500\n","Recall (weighted): 0.5000\n","F1-score (weighted): 0.3333\n"]}]},{"cell_type":"markdown","metadata":{"id":"26399851"},"source":["## Summary:\n","\n","### Data Analysis Key Findings\n","\n","*   A small dataset of 10 text examples with corresponding \"positive\", \"negative\", and \"neutral\" labels was created.\n","*   The DistilBERT tokenizer (`distilbert-base-uncased`) and a sequence classification model (`AutoModelForSequenceClassification`) were successfully loaded.\n","*   The text data was tokenized, numerical labels were created based on unique labels, and both were converted into PyTorch tensors.\n","*   The data was split into training (8 samples) and validation (2 samples) sets.\n","*   Training parameters were defined: 5 epochs, a learning rate of \\$5e-5\\$, and a batch size of 16.\n","*   The `AutoModelForSequenceClassification` model inherently includes the necessary classification head, so no separate definition was required.\n","*   A training loop was successfully implemented using a `DataLoader` and `AdamW` optimizer, showing a decrease in average training loss over the 5 epochs.\n","*   The model was evaluated on the validation set, resulting in an accuracy, precision, recall, and F1-score of 0.0000.\n","\n","### Insights or Next Steps\n","\n","*   The extremely low evaluation metrics suggest that the model did not learn effectively from the very small dataset. This could be due to the limited number of samples (especially in the validation set) and potentially the complexity of the task relative to the data size.\n","*   To improve performance, the next steps should involve using a significantly larger and more diverse dataset for training and validation. Additionally, consider hyperparameter tuning or using different pre-trained models.\n"]}]}