{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKfYsDVT5BHQb0gkVjlOkl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcmachicao/deep_learning_2025_curso/blob/main/S6__tecnicas_modernas_agilizacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import wandb"
      ],
      "metadata": {
        "id": "LeLFJmUnpzM6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 1. Setup and Dummy Data\n",
        "# -------------------------\n",
        "wandb.login()  # or wandb.init(anonymous=\"allow\") for demo mode\n",
        "\n",
        "torch.manual_seed(0)\n",
        "X = torch.randn(2000, 20)\n",
        "y = (X.sum(dim=1) > 0).long()\n",
        "train_loader = DataLoader(TensorDataset(X, y), batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "h3RtXL7Kp4A1",
        "outputId": "61b743e1-b578-4678-fbc2-29b45b8740b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgdmk\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClD6vM_YqlcP",
        "outputId": "b5e839ba-8a1b-48aa-b368-e1f609ceb760"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.1258, -1.1524, -0.2506,  ..., -1.6959,  0.5667,  0.7935],\n",
              "        [ 0.5988, -1.5551, -0.3414,  ...,  0.1124,  0.6408,  0.4412],\n",
              "        [-0.1023,  0.7924, -0.2897,  ...,  0.7440,  1.5210,  3.4105],\n",
              "        ...,\n",
              "        [ 2.1509, -0.4035, -0.3132,  ...,  1.3783,  0.2739, -0.1737],\n",
              "        [-1.3889, -2.2144, -0.3373,  ..., -0.5062, -0.6107, -0.2559],\n",
              "        [ 0.4154, -1.4043,  3.4601,  ...,  0.0167,  1.2206,  1.0346]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w34TMjHsSbO",
        "outputId": "71b5480e-afc6-49e2-bd65-91c793ad9204"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 1,  ..., 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3f2ff0e"
      },
      "source": [
        "`nn.BatchNorm1d` in PyTorch is a type of normalization layer applied to inputs that are typically mini-batches of 1D data (like in a fully connected network layer).\n",
        "\n",
        "It works by normalizing the activations of the previous layer for each mini-batch. This involves calculating the mean and variance of the activations within the batch and then scaling and shifting the normalized values using learnable parameters (gamma and beta).\n",
        "\n",
        "The benefits of using batch normalization include:\n",
        "- **Improved training stability:** Reduces the impact of changes in the distribution of activations across layers.\n",
        "- **Faster convergence:** Allows for higher learning rates.\n",
        "- **Regularization:** Can sometimes act as a mild regularizer, reducing the need for techniques like dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74a0a15f"
      },
      "source": [
        "While both batch normalization and dropout can act as regularizers, their mechanisms differ:\n",
        "\n",
        "- **Dropout:** Randomly sets a fraction of neurons to zero during training, forcing the network to be less reliant on any single neuron. This explicitly reduces the complexity of the network during each training step.\n",
        "\n",
        "- **Batch Normalization:** Introduces noise by normalizing with mini-batch statistics (mean and variance) which vary across batches. This means the network sees slightly different inputs for the same data point depending on the batch it's in. This \"noise\" can have a regularizing effect, making the network more robust.\n",
        "\n",
        "So, while both can reduce overfitting, batch normalization doesn't necessarily make \"all neurons matter\" in the same way dropout does by explicitly dropping them out. Instead, it makes the network less sensitive to the exact values of individual activations due to the batch-wise normalization. It can sometimes reduce the *need* for dropout because it offers a similar benefit of making the network more robust to small changes in activations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 2. Model Definitions\n",
        "# -------------------------\n",
        "class NormalizedMLP(nn.Module):\n",
        "    def __init__(self, norm_type='batch'):\n",
        "        super().__init__()\n",
        "        if norm_type == 'batch':\n",
        "            norm_layer = nn.BatchNorm1d(64)\n",
        "        elif norm_type == 'layer':\n",
        "            norm_layer = nn.LayerNorm(64)\n",
        "        elif norm_type == 'group':\n",
        "            norm_layer = nn.GroupNorm(4, 64)\n",
        "        else:\n",
        "            norm_layer = nn.Identity()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(20, 64),\n",
        "            norm_layer,\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "8Hi99N_Oqb_p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6gfyhJd0pw4D"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# 3. Training Function\n",
        "# -------------------------\n",
        "def train_model(norm_type='none', epochs=10):\n",
        "    wandb.init(project=\"dl-training-techniques\", name=f\"{norm_type}_norm\", reinit=True)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = NormalizedMLP(norm_type).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    warmup_steps = total_steps // 10\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for step, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Mixed precision context\n",
        "            with torch.cuda.amp.autocast(enabled=True):\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Learning rate warmup\n",
        "            current_step = epoch * len(train_loader) + step\n",
        "            if current_step < warmup_steps:\n",
        "                warmup_lr = 1e-3 * (current_step + 1) / warmup_steps\n",
        "                for g in optimizer.param_groups:\n",
        "                    g['lr'] = warmup_lr\n",
        "            else:\n",
        "                scheduler.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Log to W&B\n",
        "            wandb.log({\n",
        "                \"loss\": loss.item(),\n",
        "                \"lr\": optimizer.param_groups[0]['lr'],\n",
        "                \"epoch\": epoch,\n",
        "            })\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"[{norm_type}] Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 4. Run Experiments\n",
        "# -------------------------\n",
        "for norm in ['none', 'batch', 'layer', 'group']:\n",
        "    train_model(norm_type=norm, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YtMaPJXqr5BL",
        "outputId": "901cc1d4-7230-423b-d549-ee6900f12127"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251015_210346-c7p6rnqt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gdmk/dl-training-techniques/runs/c7p6rnqt' target=\"_blank\">none_norm</a></strong> to <a href='https://wandb.ai/gdmk/dl-training-techniques' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gdmk/dl-training-techniques' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gdmk/dl-training-techniques/runs/c7p6rnqt' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques/runs/c7p6rnqt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2448415130.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/tmp/ipython-input-2448415130.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[none] Epoch 1/5 - Loss: 0.6292\n",
            "[none] Epoch 2/5 - Loss: 0.5011\n",
            "[none] Epoch 3/5 - Loss: 0.3974\n",
            "[none] Epoch 4/5 - Loss: 0.3391\n",
            "[none] Epoch 5/5 - Loss: 0.3162\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▃▃▃▃▃▃▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆████████</td></tr><tr><td>loss</td><td>████▇▇▇▆▆▆▆▅▆▆▅▅▅▅▄▄▄▃▂▃▃▃▃▂▃▃▂▂▂▂▃▂▂▂▁▁</td></tr><tr><td>lr</td><td>▁▄▅▇███████▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.30785</td></tr><tr><td>lr</td><td>2e-05</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">none_norm</strong> at: <a href='https://wandb.ai/gdmk/dl-training-techniques/runs/c7p6rnqt' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques/runs/c7p6rnqt</a><br> View project at: <a href='https://wandb.ai/gdmk/dl-training-techniques' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251015_210346-c7p6rnqt/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251015_210348-5pd5w97e</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gdmk/dl-training-techniques/runs/5pd5w97e' target=\"_blank\">batch_norm</a></strong> to <a href='https://wandb.ai/gdmk/dl-training-techniques' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gdmk/dl-training-techniques' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gdmk/dl-training-techniques/runs/5pd5w97e' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques/runs/5pd5w97e</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[batch] Epoch 1/5 - Loss: 0.6304\n",
            "[batch] Epoch 2/5 - Loss: 0.4601\n",
            "[batch] Epoch 3/5 - Loss: 0.3553\n",
            "[batch] Epoch 4/5 - Loss: 0.3054\n",
            "[batch] Epoch 5/5 - Loss: 0.2850\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆█████████</td></tr><tr><td>loss</td><td>▇██▇▆▅▅▄▅▅▃▃▃▃▃▂▂▃▂▂▂▃▂▃▂▁▂▂▁▂▂▂▂▃▁▂▂▁▁▄</td></tr><tr><td>lr</td><td>▂▃▆▇█████████▇▇▇▇▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.44827</td></tr><tr><td>lr</td><td>2e-05</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">batch_norm</strong> at: <a href='https://wandb.ai/gdmk/dl-training-techniques/runs/5pd5w97e' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques/runs/5pd5w97e</a><br> View project at: <a href='https://wandb.ai/gdmk/dl-training-techniques' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251015_210348-5pd5w97e/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251015_210350-vxl12x8r</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gdmk/dl-training-techniques/runs/vxl12x8r' target=\"_blank\">layer_norm</a></strong> to <a href='https://wandb.ai/gdmk/dl-training-techniques' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gdmk/dl-training-techniques' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gdmk/dl-training-techniques/runs/vxl12x8r' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques/runs/vxl12x8r</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[layer] Epoch 1/5 - Loss: 0.6854\n",
            "[layer] Epoch 2/5 - Loss: 0.4554\n",
            "[layer] Epoch 3/5 - Loss: 0.3268\n",
            "[layer] Epoch 4/5 - Loss: 0.2716\n",
            "[layer] Epoch 5/5 - Loss: 0.2506\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆████████</td></tr><tr><td>loss</td><td>▇████▇▆▅▆▅▅▅▄▄▄▃▃▂▃▂▃▃▂▂▂▂▂▂▃▂▂▁▂▂▂▁▂▂▂▂</td></tr><tr><td>lr</td><td>▁▂▃▄▇███████▇▇▇▆▆▆▆▆▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.23876</td></tr><tr><td>lr</td><td>2e-05</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">layer_norm</strong> at: <a href='https://wandb.ai/gdmk/dl-training-techniques/runs/vxl12x8r' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques/runs/vxl12x8r</a><br> View project at: <a href='https://wandb.ai/gdmk/dl-training-techniques' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251015_210350-vxl12x8r/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251015_210353-fwplqfn9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gdmk/dl-training-techniques/runs/fwplqfn9' target=\"_blank\">group_norm</a></strong> to <a href='https://wandb.ai/gdmk/dl-training-techniques' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gdmk/dl-training-techniques' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gdmk/dl-training-techniques/runs/fwplqfn9' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques/runs/fwplqfn9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[group] Epoch 1/5 - Loss: 0.6614\n",
            "[group] Epoch 2/5 - Loss: 0.4830\n",
            "[group] Epoch 3/5 - Loss: 0.3622\n",
            "[group] Epoch 4/5 - Loss: 0.2969\n",
            "[group] Epoch 5/5 - Loss: 0.2685\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▃▃▅▅▅▆▆▆▆▆▆▆████████</td></tr><tr><td>loss</td><td>█▇▇██▆▆▆▆▆▆▆▅▆▅▄▄▅▄▅▃▄▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>lr</td><td>▂▄▆▆██████████▇▇▇▇▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.19215</td></tr><tr><td>lr</td><td>2e-05</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">group_norm</strong> at: <a href='https://wandb.ai/gdmk/dl-training-techniques/runs/fwplqfn9' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques/runs/fwplqfn9</a><br> View project at: <a href='https://wandb.ai/gdmk/dl-training-techniques' target=\"_blank\">https://wandb.ai/gdmk/dl-training-techniques</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251015_210353-fwplqfn9/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
