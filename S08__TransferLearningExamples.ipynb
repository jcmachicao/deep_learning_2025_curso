{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN12p+bAB61XgfTTydBSCMy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcmachicao/knowledge_engineering/blob/main/U3__TransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modern Deep Learning Training Techniques in PyTorch\n",
        "# --------------------------------------------------\n",
        "### This notebook demonstrates three advanced strategies:\n",
        "### 1. Transfer Learning & Fine-Tuning\n",
        "### 2. Self-Supervised Learning (Contrastive & Masked Prediction)\n",
        "### 3. Curriculum & Active Learning"
      ],
      "metadata": {
        "id": "LKZBfuvPE0Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import wandb# Initialize Weights & Biases (you can disable if running offline)"
      ],
      "metadata": {
        "id": "xOVVvbS3Eyau"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"modern-dl-training\", name=\"demo_notebook\", reinit=True)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "xLbOZ7btE8jP",
        "outputId": "3b23d899-6c5a-4a2f-cf7f-b7993cb35130"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgdmk\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251016_123834-lu3qx1q7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gdmk/modern-dl-training/runs/lu3qx1q7' target=\"_blank\">demo_notebook</a></strong> to <a href='https://wandb.ai/gdmk/modern-dl-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gdmk/modern-dl-training' target=\"_blank\">https://wandb.ai/gdmk/modern-dl-training</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gdmk/modern-dl-training/runs/lu3qx1q7' target=\"_blank\">https://wandb.ai/gdmk/modern-dl-training/runs/lu3qx1q7</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cb42e9e"
      },
      "source": [
        "The `wandb.init()` function automatically creates a new project in your Weights & Biases account if a project with the specified name (`\"modern-dl-training\"` in this case) doesn't already exist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b580b53"
      },
      "source": [
        "The name of the final layer (`fc` in this case) is specific to the particular pre-trained model architecture (like ResNet in this example). Different model architectures might use different names for their final layers.\n",
        "\n",
        "There isn't a single universal glossary that lists all layer names for all possible models. However, you can inspect the model's structure programmatically to see the names of its layers.\n",
        "\n",
        "For example, you could print the model to see its layers and their names:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d5e6377"
      },
      "source": [
        "In many pre-trained models from libraries like torchvision, the final layer is often a fully connected layer (also known as a dense layer) that is used for classification. This layer is typically named `fc`.\n",
        "\n",
        "When performing transfer learning, we often want to adapt the pre-trained model to a new task with a different number of output classes. We do this by replacing this final `fc` layer with a new fully connected layer that has the desired number of output neurons (in this example, 3 for a 3-class classification).\n",
        "\n",
        "By freezing the other layers (`for p in model.parameters(): p.requires_grad = False`), we keep the learned features from the pre-trained model and only train the new `fc` layer on the new data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# 1. TRANSFER LEARNING & FINE-TUNING\n",
        "# --------------------------------------------------\n",
        "\n",
        "def transfer_learning_example():\n",
        "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "\n",
        "    # Freeze all layers\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Replace final layer for 3-class classification\n",
        "    model.fc = nn.Linear(model.fc.in_features, 3)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Dummy data\n",
        "    X = torch.randn(32, 3, 224, 224)\n",
        "    y = torch.randint(0, 3, (32,))\n",
        "\n",
        "    outputs = model(X)\n",
        "    loss = criterion(outputs, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    wandb.log({\"transfer_learning_loss\": loss.item()})\n",
        "    print(\"[Transfer Learning] Loss:\", loss.item())"
      ],
      "metadata": {
        "id": "E2WLHNSUFCG1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8SK-XqNIDzu1"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------\n",
        "# 2. SELF-SUPERVISED LEARNING\n",
        "# --------------------------------------------------\n",
        "\n",
        "def contrastive_learning_example():\n",
        "    # Contrastive learning (SimCLR style, minimal)\n",
        "    z1 = F.normalize(torch.randn(8, 128), dim=1)\n",
        "    z2 = F.normalize(torch.randn(8, 128), dim=1)\n",
        "\n",
        "    sim_matrix = torch.mm(z1, z2.T) / 0.5  # temperature scaling\n",
        "    labels = torch.arange(8)\n",
        "    loss = F.cross_entropy(sim_matrix, labels)\n",
        "\n",
        "    wandb.log({\"contrastive_loss\": loss.item()})\n",
        "    print(\"[Contrastive Learning] Loss:\", loss.item())\n",
        "\n",
        "\n",
        "def masked_prediction_example():\n",
        "    x = torch.randint(0, 100, (4, 10))\n",
        "    mask = torch.rand_like(x.float()) < 0.3\n",
        "    x_masked = x.clone()\n",
        "    x_masked[mask] = 0  # mask token\n",
        "\n",
        "    model = nn.Embedding(100, 32)\n",
        "    linear = nn.Linear(32, 100)\n",
        "\n",
        "    emb = model(x_masked)\n",
        "    pred = linear(emb)\n",
        "\n",
        "    loss = nn.CrossEntropyLoss()(pred[mask], x[mask])\n",
        "\n",
        "    wandb.log({\"masked_loss\": loss.item()})\n",
        "    print(\"[Masked Prediction] Loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# 3. CURRICULUM LEARNING\n",
        "# --------------------------------------------------\n",
        "\n",
        "def curriculum_learning_example():\n",
        "    X = torch.linspace(-5, 5, 500).unsqueeze(1).to(device)\n",
        "    y = torch.sin(X) + 0.1 * torch.randn_like(X) # y is created on the same device as X\n",
        "    y = y.to(device) # Ensure y is on the device\n",
        "\n",
        "    # Modified model definition to potentially fix shape mismatch\n",
        "    model = nn.Sequential(nn.Linear(1, 100), nn.ReLU(), nn.Linear(100, 1)).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(5):\n",
        "        difficulty = 1 + epoch  # progressively expand input range\n",
        "\n",
        "        # Aqui se ajusta las formas de las matrices para que sean compatibles para la multiplicación\n",
        "        mask = (X.abs() < difficulty).view(-1)\n",
        "        X_batch, y_batch = X[mask], y[mask]\n",
        "        X_batch = X_batch.view(-1, 1)\n",
        "        y_batch = y_batch.view(-1, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(X_batch), y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        wandb.log({\"curriculum_loss\": loss.item(), \"epoch\": epoch})\n",
        "        print(f\"[Curriculum] Epoch {epoch} | Range [-{difficulty}, {difficulty}] | Loss={loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "1Vwf9VSZFJsW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# 4. ACTIVE LEARNING (conceptual demo)\n",
        "# --------------------------------------------------\n",
        "\n",
        "def active_learning_example():\n",
        "    probs = torch.softmax(torch.randn(10, 3), dim=1)\n",
        "    entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
        "    topk = torch.topk(entropy, k=3)\n",
        "    wandb.log({\"avg_entropy\": entropy.mean().item()})\n",
        "    print(\"[Active Learning] Most uncertain samples:\", topk.indices.tolist())"
      ],
      "metadata": {
        "id": "akgYmBYeFNZa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# Run all demos\n",
        "# --------------------------------------------------\n",
        "transfer_learning_example()\n",
        "contrastive_learning_example()\n",
        "masked_prediction_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lOuGuFbFRIA",
        "outputId": "c7278873-633c-417f-8377-c8c7f2c0207d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 76.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Transfer Learning] Loss: 1.2010308504104614\n",
            "[Contrastive Learning] Loss: 1.9420629739761353\n",
            "[Masked Prediction] Loss: 4.663577079772949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "curriculum_learning_example()\n",
        "active_learning_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKEZbgUtIOcM",
        "outputId": "5fcbc406-d7b3-4687-bf05-23400c95e076"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Curriculum] Epoch 0 | Range [-1, 1] | Loss=0.8028\n",
            "[Curriculum] Epoch 1 | Range [-2, 2] | Loss=1.0259\n",
            "[Curriculum] Epoch 2 | Range [-3, 3] | Loss=0.4883\n",
            "[Curriculum] Epoch 3 | Range [-4, 4] | Loss=0.8958\n",
            "[Curriculum] Epoch 4 | Range [-5, 5] | Loss=1.8250\n",
            "[Active Learning] Most uncertain samples: [2, 8, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "ZpxyHSjaFTA2",
        "outputId": "b8dbdd39-e895-4699-d3b5-6cf9434f664e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_entropy</td><td>▁</td></tr><tr><td>contrastive_loss</td><td>▁</td></tr><tr><td>curriculum_loss</td><td>▃▄▁▃█</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>masked_loss</td><td>▁</td></tr><tr><td>transfer_learning_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_entropy</td><td>0.86986</td></tr><tr><td>contrastive_loss</td><td>1.94206</td></tr><tr><td>curriculum_loss</td><td>1.82495</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>masked_loss</td><td>4.66358</td></tr><tr><td>transfer_learning_loss</td><td>1.20103</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">demo_notebook</strong> at: <a href='https://wandb.ai/gdmk/modern-dl-training/runs/lu3qx1q7' target=\"_blank\">https://wandb.ai/gdmk/modern-dl-training/runs/lu3qx1q7</a><br> View project at: <a href='https://wandb.ai/gdmk/modern-dl-training' target=\"_blank\">https://wandb.ai/gdmk/modern-dl-training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251016_123834-lu3qx1q7/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}